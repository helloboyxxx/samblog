[{"content":"⚠️THIS NOTE IS NOT FINISHED YET TODO: stirling number definitions stirling number of the first and the second type bell number Question Can you find a closed formula for the sum of the $p$-th powers of the first $n$ positive integers? That is, we need a closed formulat for the sequence:\n$$ h_n=a_p n^p+a_{p-1} n^{p-1}+\\cdots+a_1 n+a_0, \\quad(n \\geq 0) $$\nSolution\nThe method is to use difference sequences. We already know the following:\n$$ 1^3 + 2^3 + \\ldots + n^3 = (1+2+\\ldots + n)^2 $$ Now we want a new method. First, construct a difference table for $n^3$.\nLet\u0026rsquo;s verify the following equation:\n$$ 1^3 + \\ldots + n^3 = 0 \\cdot \\binom{n+1}{1} + 1 \\cdot \\binom{n+1}{2} + 6 \\cdot \\binom{n+1}{3} + 6 \\cdot \\binom{n+1}{4} $$\nUse Pascal\u0026rsquo;s formula for the last two terms. Then expand them to verify that this equation is true.\nDefinition\nLet $(h_n)_{n \\geq 0}$ be a sequence of numbers. We define a new sequence:\n$$ \\Delta h_0, \\Delta h_1, \\Delta h_2, \\ldots, \\Delta h_n, \\ldots $$ call the first-order difference sequence of $(h_n)_{n \\geq 0}$ by:\n$$ \\Delta h_n = h_{n+1} - h_n $$\nDiscrete derivative.\nDefinition\nThe second-order difference sequence of $(h_n)_{n \\ge 0}$ is defined by\n$$ \\Delta^2 h_0, \\Delta^2 h_1, \\Delta^2 h_2, \\ldots, \\Delta^2 h_n, \\ldots, $$ where\n$$ \\Delta^2 h_n = \\Delta(\\Delta h_n) = \\Delta h_{n+1} - \\Delta h_{n}. $$\nThen we can construct the difference table using this new notation:\nTheorem 8.2.1 Let the general term of a sequence be a polynomial of degree $p$ in $n$: $$ h_n = a_pn^p + a_{p-1}n^{p-1} + \\ldots + a_1n + a_0, \\quad(n\\geq 0) $$ Then, $\\Delta^{p+1} h_n = 0$ for all $n \\geq 0$.\nProof\nInduction on $p$. IH:\n$$ \\Delta^{p+1}h_n = 0, \\forall n \\geq 0, \\forall p \\text{-degree sequence } (h_n)_{n\\geq 0} $$\nBase case: $p = 0$, then $(h_n)_{n \\geq 0}$ is a constant sequence $\\implies \\Delta h_n = h_{n+1} - h_n = 0$.\nInduction step: Assume this holds for $p-1, p \\geq 1$. Let\u0026rsquo;s show that this holds for $p$ as well. We have: $$ \\begin{align*} \\Delta h_n = h_{n+1} - h_n \u0026amp;= \\left(a_p(n+1)^p+a_{p-1}(n+1)^{p-1}+\\cdots+a_1 n+a_0\\right)\\\\ \u0026amp; \\quad -\\left(a_p n^p+a_{p-1} n^{p-1}+\\cdots+a_1 n+a_0\\right)\\\\ \u0026amp;= (a_p(n+1)^p - a_p n^p) + \\text{polynomial with degree }p-1 \\end{align*} $$ By IH, the polynomial with degree $p-1$ goes to zero. Since we want to show that $\\Delta^{p+1} h_n$ still goes to zero, we need to show that:\n$$ a_p(n+1)^p - a_p n^p = 0 $$\nNow, to solve this, we can use binomial theorm:\n$$ \\begin{align*} a_p(n+1)^p - a_p n^p \u0026amp;= a_p\\left( \\sum_{i=0}^{p}\\binom{p}{i}n^{i} \\right) - a_pn^p\\\\ \u0026amp;= a_p\\left( \\sum_{i=0}^{p-1} \\binom{p}{i}n^i \\right) + a_p n^p - a_pn^p\\\\ \u0026amp;= a_p\\left( \\sum_{i=0}^{p-1} \\binom{p}{i}n^i \\right) \\end{align*} $$\nWhich means that this polynomial also has degree $p-1$. Combining with the term above, we have: $$ \\Delta ^{p+1} = \\Delta^p(\\Delta h_n) = \\Delta^p(\\text{Polynomial with degree }p-1) = 0 $$ $\\blacksquare$\nLemma Linearity of difference\nLet $(f_n)_{n\\geq 0}$ and $(g_n)_{n\\geq 0}$ be two sequences of numbers. Let $(h_n)_{n\\geq 0}$ be a sequence defined by: $$ h_n = g_n + f_n $$ For every $p \\geq 0$ we have: $$ \\Delta^p h_n = \\Delta^p g_n + \\Delta^p f_n $$ Proof\nIt suffices to show that: $$ \\Delta (f_n + g_n) = \\Delta f_n + \\Delta g_n $$ Expand this and use commutative property to show this:\n$$ \\begin{align*} \\Delta h_n \u0026amp;=h_{n+1}-h_n \\\\ \u0026amp;=\\left(g_{n+1}+f_{n+1}\\right)-\\left(g_n+f_n\\right) \\\\ \u0026amp;=\\left(g_{n+1}-g_n\\right)+\\left(f_{n+1}-f_n\\right) \\\\ \u0026amp;=\\Delta g_n+\\Delta f_n \\end{align*} $$\nQuestion\nCan we recover a sequence by knowing the $0^{\\text{th}}$ diagonal of its difference table?\nNote that the $0-$diagonal has the terms: $$ \\Delta^0 h_0, \\Delta^1 h_0, \\Delta^2 h_0, \\Delta^3 h_0, \\ldots $$\nLemma\nThe general term of the sequence $\\left(h_n\\right)_{n \\geq 0}$ such that the Oth diagonal of its difference table is\n$$ \\underbrace{000 \\cdots 0}_{p \\text { zeros }} 1000 \\cdots $$ equals to\n$$ h_n= \\binom{n}{p} $$ Proof\nFrom the 0-diagonal, we can first try to recover the 1-diagonal, 2-diagonal\u0026hellip; and so on:\nSay the $1$ is the fifth number to appear in the 0-diagonal, then it also appears at the fifth position in the first row. All the other terms in the first row are zero. We are then looking for a sequence that has the following properties: $$ h_0 = h_1 = h_2 = h_3 = h_4 = 0, h_1 = 1, $$ Thus, if $h_n$ is sequence of degree 4, it has roots at $0, 1, 2, 3$. Write out out simple guess, we have: $$ h_n = c \\cdot n \\cdot (n-1) \\cdot (n-2) \\cdot (n-3) $$ for some constant $c$. To satisfies the term $h_1 = 1$, $c = \\frac{1}{4!}$. We discover that $h_n = \\binom{n}{4}$. More generally, $h_n = \\binom{n}{p}$ if there are $p$ zeros in the beginning of the 0-diagonal.\nIntuition: using the linearity property of differences and the fact that the 0th diagonal of a difference table determines the entire difference table, and hence the sequence itself, we obtain the next theorem.\nTheorem 8.2.2 The general term of the sequence whose difference table has its 0th diagonal equal to $$ c_0, c_1, c_2, \\ldots, c_p, 0,0,0, \\ldots, \\quad \\text { where } c_p \\neq 0 $$ is a polynomial in $n$ of degree $p$ satisfying $$ h_n=c_0\\left(\\begin{array}{l} n \\ 0 \\end{array}\\right)+c_1\\left(\\begin{array}{l} n \\ 1 \\end{array}\\right)+c_2\\left(\\begin{array}{l} n \\ 2 \\end{array}\\right)+\\cdots+c_p\\left(\\begin{array}{l} n \\ p \\end{array}\\right) $$\nNow, we can back to the initial question: can we find a closed formula for the sum of the $p$-th powers of the first $n$ positive integers? Now we have the tools.\nWe can first write out the difference table. Then, we look at the 0-diagonal, and we reconstruct the sequence, but in the form we see in Theorem 8.2.2. Here is an example:\nExercise\nConsider the sequence with general term $$ h_n=n^3+3 n^2-2 n+1, \\quad(n \\geq 0) $$ The 0-diagonal is $1,2,12,6,0,0, \\ldots$. By using Theorem 8.2.2, we get: $$ h_n = \\binom{n}{0} + 2\\binom{n}{1} +12\\binom{n}{2} + 6\\binom{n}{3} $$ Summing them up: $$ \\sum_{k=0}^{n} h_k = \\sum_{k=0}^{n}\\binom{k}{0} + 2\\sum_{k=0}^{n}\\binom{k}{1} +12\\sum_{k=0}^{n}\\binom{k}{2} + \\sum_{k=0}^{n}6\\binom{k}{3} $$ By hokey-stick identity, this turns to: $$ \\sum_{k=0}^{n} h_k = \\binom{n+1}{1} + 2 \\binom{n+1}{2} + 12 \\binom{n+1}{3} + 6\\binom{n+1}{4} $$\nThe general expression of what we have calculated.\nTheorem Theorem 8.2.3 Assume that the sequence $h_0, h_1, h_2, \\ldots, h_n, \\ldots$ has a difference table whose 0 th diagonal equals $$ c_0, c_1, c_2, \\ldots, c_p, 0,0, \\ldots $$\nThen $$ \\sum_{k=0}^n h_k=c_0\\left(\\begin{array}{c} n+1 \\ 1 \\end{array}\\right)+c_1\\left(\\begin{array}{c} n+1 \\ 2 \\end{array}\\right)+\\cdots+c_p\\left(\\begin{array}{c} n+1 \\ p+1 \\end{array}\\right) $$\nProof is the same as our calculation above.\nNext, we discuss the sequence $$ h_n = n^p $$ for some constant $p$.\nBell numbers\nTheorem If $p \\geq 1$, then $$ B_p = \\binom{p-1}{0} B_0 + \\binom{p-1}{1}B_1 + \\ldots + \\binom{p-1}{p-1}B_{p-1} $$ Proof\nHere is the construction:\nThere are $p$ elements in total, and some boxes.\nFix the element $p$. Fix $t \\in \\set{0, \\ldots, p-1}$ which chooses the number of elements in the box containning $p$. Let $a_t = $ number of the choices of elements which are in the box with $p$, which equals $\\binom{p-1}{t}$\nIt remains to partition $p-(t-1)$ elements into indistinguishable non-empty boxes $B_{p-t-1}$ ways. Writing out the relation we get:\nStirling number of the first kind ==\u0026hellip;==\n","permalink":"https://ohuro.me/posts/413_difference-sequence/","summary":"⚠️THIS NOTE IS NOT FINISHED YET TODO: stirling number definitions stirling number of the first and the second type bell number Question Can you find a closed formula for the sum of the $p$-th powers of the first $n$ positive integers? That is, we need a closed formulat for the sequence:\n$$ h_n=a_p n^p+a_{p-1} n^{p-1}+\\cdots+a_1 n+a_0, \\quad(n \\geq 0) $$\nSolution\nThe method is to use difference sequences. We already know the following:","title":"413_difference Sequence"},{"content":"Example sums of cubes\nSolve $h_n=h_{n-1}+n^3,(n \\geq 1)$ and $h_0=0$.\nSolution\nNote that this is not a homegenous recurrence relation.\nUnrolling gives us: $$ \\begin{align*} h_{n-1} \u0026amp;= (n-1)^3 + h_{n-2}\\\\ \\implies h_n \u0026amp;= n^3 + (n-1)^3 + h_{n-2} \\end{align*} $$ And we can keep doing this. Using this as the intuition, we guess the closed form: $$ h_n = n^3 + (n-1)^3 + \\ldots + 2^3 + 1 $$ To formally prove this, we can use induction! Assume it holds for some $n \\geq 1$, we want to show that $h_{n+1} = h_n + (n+1)^3 = 1 + 2^3 + \\ldots + (n+1)^3$.\nAfter proving this formula, we are now trying to come up with a nice and clean formula for finding the sum of cubes. Think of adding up cubes that length of their edges are $1, 2, \\ldots, n$.\nLemma\n$$ 1^3 + 2^3 + \\ldots + n^3 = (1 + 2 + \\ldots + n)^2 $$\nProof by induction\nThis is obviously true for $n = 1$. Then assume this identity is true for some $n \\in \\mathbb{N}$. Let\u0026rsquo;s prove $n+1$ case. $$ \\begin{align*} 1 + 2^3 + \\ldots + n^3 + (n+1)^3 \u0026amp;\\overset{I.H.}{=} (1 + \\ldots + n)^2 + (n+1)^3\\\\ \u0026amp;\\overset{?}{=}(1 + 2 + \\ldots + n + (n+1))^2\\\\ \\implies (1 + 2 + \\ldots + n + (n+1))^2 \u0026amp;= (1 + 2 + \\ldots + n)^2 + (n+1)^2 + 2(1 + \\ldots + n)\\cdot (n+1)\\\\ \\text{Need to check: }\\\\ (n+1)^3 \u0026amp;\\overset{?}{=} (n+1)^2 + 2(1 + \\ldots + n)\\cdot (n+1) \\end{align*} $$ Start from the right side: $$ \\begin{align*} (n+1)^2 + 2(1 + \\ldots + n)\\cdot (n+1) \u0026amp;= (n+1)(2(1+\\ldots+n) + (n+1))\\\\ \u0026amp;= (n+1)\\left( 2\\binom{n+1}{2} + (n+1) \\right)\\\\ \u0026amp;= (n+1)\\left( n(n+1)+(n+1) \\right)\\\\ \u0026amp;= (n+1)^3 \\end{align*} $$ $\\blacksquare$\nI have learnt an interesting way of thinking the meaning of $(1+2+\\ldots + n)$ from this proof.\nSay we have a complete graph $K_n$. We need to count the number of edges. Pick one vetex and count all the edges connected to it, which has $n$. Then we throw this vertex away, we are left with a complete graph $K_{n-1}$. Repeating this method for $n$ times, can count all the edges. We also know that the number of edges in $K_n$ is $\\binom{n}{2}$, so the sum$(1 + 2 + \\ldots + n) = \\binom{n}{2}$.\nExercise $$ h_n = 2h_{n-1} + 1 $$\nExercise\n$\\text { Solve } h_n=3 h_{n-1}-4 n,(n \\geq 1) \\text { and } h_0=2$.\nSolution\nPut all the terms related to $h_n$ to the left side of the equation helps. $$ h_n - 3h_{n-1} = -4n $$\nFor this one, let\u0026rsquo;s first consider a homogeneous recurrence relation without the $-4n$ term. $$ a_n = 3a_{n-1} \\implies a_n - 3a_{n-1} = 0 $$ It is easy to find out that the closed form for this HRR is $a_n = 3^n$.\nWe then try to guess a particular solution to the NHRR, but we don\u0026rsquo;t care about the initial condition $h_0$. Since the right side $-4n$ is a degree 1 polynomial, we may guess the particular solution is in the form: $$ b_n = sn + r $$ for some constant $s$ and $r$. Since we hope that $b_n$ satisfies the recurrence relation $b_n = 3b_{n-1}-4n$, we can solve for $s$ and $r$: $$ \\begin{align*} b_n \u0026amp;= 3(s(n-1)+r) - 4n\\\\ \u0026amp;= 3(sn-s+r)-4n\\\\ \u0026amp;= (3s-4)n + 3(r-s) = sn+r\\\\ \u0026amp;\\implies \\begin{cases} s=2\\\\ r=3 \\end{cases} \\end{align*} $$\nSolution to homogeneous recurrence relation: $$ a_n = 3^n $$\nParticular solution:\n$$ b_n = 2n + 3 $$\nCombine them together to get: $$ \\begin{align*} h_n \u0026amp;= c \\cdot a_n + b_n = 3h_{n-1} - 4n\\\\ h_n \u0026amp;= c(3^n) + (2n+3), \\quad h_0 = 2 \\end{align*} $$\nThere is one important thing to learn here. Question: why do we want to combine $a_n$ and $b_n$ together? Why we multiply $a_n$ by a constant $c$ while we don\u0026rsquo;t do this for $b_n$? The answer is that the\nUsing the initial condition, we solve for $c = -1$. Therefore, the full solution to the initial NHRR is: $$ h_n = 2n+3-3^n $$ $\\blacksquare$\nTips for finding general solutions of non-homogeneous recurrence relation Essentially, to find the general solution of a non-homogeneous recurrence relation, we need to:\n(1) Find the general solution of the associated homogeneous relation\n(2) Find a particular solution of the nonhomogeneous relation.\n(3) Combine the general solution and the particular solution, and determine values of the constants arising in the general solution so that the combined solution satisfies the initial conditions.\nThe main difficulty is finding a particular solution in step (2). For some nonhomogeneous parts, there are certain types of particular solutions to try.\nConsider a nonhomogeneous recurrence relation of the form $$ h_n=a h_{n-1}+b_n $$ These are the guess direction for possible particular solutions of $h_n$:\n(a) If $b_n$ is a polynomial of degree $k$ in $n$, then look for a particular solution $h_n$ that is also a polynomial of degree $k$ in $n$.\n(b) If $b_n$ is an exponential, then look for a particular solution that is also an exponential.\nThere are some common forms that you can try to guess:\n$f(n)$ $a_n$ $c$ $A$ $n$ $A_1n + A_0$ $n^2$ $A_2n^2 + A_1n + A_0$ $r^n$ $Ar^n$ where $c, A, r$ are constants.\n","permalink":"https://ohuro.me/posts/413_nhrr/","summary":"Example sums of cubes\nSolve $h_n=h_{n-1}+n^3,(n \\geq 1)$ and $h_0=0$.\nSolution\nNote that this is not a homegenous recurrence relation.\nUnrolling gives us: $$ \\begin{align*} h_{n-1} \u0026amp;= (n-1)^3 + h_{n-2}\\\\ \\implies h_n \u0026amp;= n^3 + (n-1)^3 + h_{n-2} \\end{align*} $$ And we can keep doing this. Using this as the intuition, we guess the closed form: $$ h_n = n^3 + (n-1)^3 + \\ldots + 2^3 + 1 $$ To formally prove this, we can use induction!","title":"413_None Homogeneous Recurrence Relation"},{"content":"Generating Functions Generating functions can be regarded as algebraic objects whose formal manipulation allows us to count the number of possibilities for a problem by means of algebra.\nDefinition\nLet $h_0, h_1, h_2, h_3, \\ldots$ be an infinite sequence of numbers. Its generating function is defined to be the infinite series: $$ g(x) = h_0 + h_1x + h_2x^2 + \\cdots + h_nx^n + \\cdots $$\nExample 1 The generating function of the infinite sequence $1, 1, 1, \\ldots$ is: $$ g(x) = 1 + x + x^2 + x^3 + \\cdots $$ for $|x| \u0026lt; 1$. This generating function $g(x)$ is the sum of a geometric series with value: $$ g(x) = \\frac{1}{1-x} $$ Solution\n$|x| \u0026lt; 1$, $$ \\begin{align*} (1-x)(1 + x + x^2 + x^3 + \\cdots + x^n) \u0026amp;= 1 - x^{n+1}\\ \\Rightarrow\\ \\lim_{n \\to \\infty}(1-x)(1 + x + x^2 + x^3 + \\cdots + x^n) \u0026amp;= \\lim_{n \\to \\infty}(1 - x^{n+1}) = 1\\ \\Rightarrow\\ \\sum_{j=0}^{\\infty}x^j = \\frac{1}{1-x} \\end{align*} $$ Example 2 Let $m$ be a positive integer. The generating function for the binomial coefficients: $$ \\binom{m}{0}, \\binom{m}{1}, \\ldots, \\binom{m}{m} $$ Solution $$ g(x) = \\binom{m}{0}x + \\binom{m}{1}x^2+ \\ldots+ \\binom{m}{m}x^m = (1+x)^m $$ By the binomial theorem.\nThink about the coefficients of generating functions Example 3 Let $k$ be an integer, and let the sequence $h_0, h_1, h_2, \\ldots$ be defined by letting $h_n$ equal the number of nonnegative integeral solutions of $x_1+x_2+\\cdots+x_k = n.$ Show that the generating function of this sequence is equal to $$ g(x) = \\frac{1}{(1-x)^k} $$\nSolution\nThe goal is to convince ourself that expanding the function $g(x) = \\frac{1}{(1-x)^k}$ give us special coefficients. (What is that? 🤔)\nSince we know that $\\frac{1}{(1-x)} = 1 + x + x^2 + \\ldots$, the expansion becomes: $$ \\begin{align*} \\frac{1}{(1-x)^k} = \\underbrace{(1 + x + x^2 + \\ldots) \\cdot (1 + x + x^2 + \\ldots) \\cdots (1 + x + x^2 + \\ldots)}_{k} \\end{align*} $$ Note that there are $k$ pairs of parenthesis, and from each pair of parentheis, we can choose a $x^i$, for some $i \\geq 0$. Multiplying these $x_i$\u0026rsquo;s, say $x^{i_1} \\cdot x^{i_2}\\cdots x^{i_k}$, which is equivalent to $x^{i_1 + i_2 + \\ldots + i_k} = x^n$ for some $n$. Now, given a fixed $n$, we can think about the coefficients of the term $x^n$\u0026hellip; Well, MAGIC !!! This is same as the number of nonnegative integral solutions of $x_1 + x_2 + \\ldots + x_k = n$. Just in different notation!\nTo make this clearer, just write out the definition of generating functions of this sequence: $$ g(x) = h_0 + h_1x + h_2x^2 + h_3x^3 + \\ldots $$ SEE? Each coefficients of $x^n$ corresponds to some choices of $x^i$ from the expansion of $\\frac{1}{(1-x)^k}$. Thus, the sum of all the terms in generating function can be simplified / is equivalent to $\\frac{1}{(1-x)^k}$ . $\\blacksquare$\nThe general trick for generating function problems is to find out an expression for these $h_n$\u0026rsquo;s. Sometimes, we make some guesses and multiply a few generating functions together, and observe that the coefficients of $x$\u0026rsquo;s in the multiplication is what we want. We will see more examples. (I mean a LOT 🙃)\nExample 4 (Read example 5 first for better understanding. Also, do compare this two.) For what sequence is $(1+x+x^2+x^3+x^4+x^5)(1+x+x^2)(1+x+x^2+x^3+x^4)$ the generating function?\nSolution\nFor this one, we are trying to construct a case such that, when writing out the generating function for them, we get this expression. This is a \u0026ldquo;reverse designing\u0026rdquo; problem. It is hard to come up with these from no where if this is the first time you see them. But, as long as we read/learn more solutions, we can be experienced enough to make correct guesses. That\u0026rsquo;s how Combinatoric works. GET USED TO IT 😢\nObserve that there are three terms multiplied together. Therefore, we may have three kinds of things. Each kind has different copies\u0026hellip; This reminds us of something called multi-set.\nConsider a multi-set: $$ S = \\set{5 \\cdot a, 2 \\cdot b, 4 \\cdot c} $$ The questions is: what is the generating function for the number of $n$-combinations of $S$? $$ g(x) = h_0 + h_1x + h_2x^2 + h_3x^3 + \\ldots $$ Where $h_n$ represents the number of different combinations using $S$. To build the connection between the initial expression and this generating function, simply try to expand it and check the coefficient of each $x^n$. It turns out that the coefficients $h_n$ give us the number of $n$-combinations using $S$. $\\blacksquare$\nExample 5 Determine the generating function for the number of $n$-combinations of apples, bananas, oranges, and pears, where in each $n$-combination, the number of apples is even, the number of bananas is odd, the number of oranges is between $0$ and $4$, and there is at least one pear.\nSolution\nIntuition is to come up to a multiplication of generating functions, where each generating function corresponds to one fruit.\nConsider:\n$(1 + x^2 + x^4 + \\ldots) \\to $ Apples\n$(x + x^3 + x^5 + \\ldots) \\to $ Bananas\n$(1 + x + x^2 + x^3 + x^4) \\to $ Oranges\n$(x + x^2 + x^3 + \\ldots) \\to $ Pears\nNote that the coeff of $x^n$ is the number of $n$-combinations of fruits.\nIf we expand the multiplication of these four terms, we get: $$ g(x) = \\sum_{a_1,\\ldots, a_4}x^{a_1} \\cdot x^{a_2} \\cdot x^{a_3} \\cdot x^{a_4} = \\sum_{n \\geq 0}h_nx^n $$ The coefficients $h_n$ in the expression above is what we want / is what we are asked for. We may first notice that finding the correct coefficients is the goal, then we try to construct the generating functions that satisfies the properties.\nApples $ = \\frac{1}{1-x^2}$ by replacing $x$ by $x^2$.\nBananas $=\\frac{x}{1-x^2}$ by multiplying apple by $x$.\nOranges $= \\frac{1-x^5}{1-x}$ since $(1-x^5) = (1-x)(1 + x + x^3 + x^4)$\nPears $= (\\frac{1}{1-x}-1) = \\frac{x}{1-x}$. Or replace 1 by $x$ in the expression $(1 + x + x^2 + x^3 + \\ldots) = \\frac{1}{1-x}$\nThen we take the multiplication. $\\blacksquare$\nExample 6 Find the number $h_n$ of bags of fruit that can be made out of apples, bananas, oranges, and pears, where, in each bag, the number of apples is even, the number of bananas is a multiple of five, the number of oranges is at most 4, and the number of pears is 0 or 1.\nSolution\nSimilar to Example 5: $$ g(x) = (1 + x^2 + x^4 + \\ldots) (1 + x^5 + x^{10} + \\ldots)(1 + x + x^2 + x^3 + x^4) (1+x) $$ Simplify it to get: $$ g(x) = \\frac{1}{(1-x)^2} $$ To calculate $h_n$ from this, we can use Taylor expansion:\nWHY?\n⚠️ Calc review: $$ \\begin{aligned} \u0026amp; f \\in \\mathcal{C}^{\\infty}, 0 \\in \\operatorname{Domain}(f) \\\\ \u0026amp; f(x)=f(0)+\\frac{f^{\\prime}(0)}{1 !} \\cdot x+\\frac{f^{\\prime \\prime}(0)}{2 !} \\cdot x^2+\\ldots \\\\ \u0026amp; f(x)=\\frac{1}{(1-x)^2} \\\\ \u0026amp; f^{\\prime}(x)=\\frac{2}{(1-x)^3} \\\\ \u0026amp; f^{\\prime \\prime}(x)=\\frac{2 \\cdot 3}{(1-x)^4} \\\\ \u0026amp; f^{(n)}(x)=\\frac{2 \\cdot 3 \\cdots(n+1)}{(1-x)^{n+2}}\\\\ \u0026amp; f^{(n+1)}(x)=\\frac{2 \\cdot 3 \\cdot(n+1)(n+2)}{(1-x)^{n+3}} \\\\ \u0026amp; \\end{aligned} $$ For $n \\in \\mathbb{N}:$ $$ \\begin{aligned} \u0026amp; f^{(n)}(0)=(n+1) ! \\\\ \u0026amp; f(x)=1+\\frac{2 !}{1 !} x+\\frac{3 !}{2 !} x^2+\\cdots \\\\ \u0026amp; =1+\\sum_{n \\geqslant 1} \\frac{f^{(n)}(0) x^n}{n !}=1+\\sum_{n \\geqslant 1}^1(n+1) x^n \\end{aligned} $$ Then our answer is simply $h_n = n+1$. $\\blacksquare$\nOR\nWe can solve this in another way: $$ \\begin{align*} \\left(\\frac{1}{1-x}\\right)^\\prime \u0026amp;= (1 + x + x^2 + \\ldots)^\\prime\\\\ \\frac{1}{(1-x)^2} \u0026amp;= 0 + 1 + 2x + 3x^2 + \\ldots \\end{align*} $$ Then we find out that $h_n = n+1$. $\\blacksquare$\nExponential generating functions So far, we defined the generating function for a sequence $\\left(h_n\\right)_{n \\geq 0}$ by using the monomials $1, x, x^2, \\ldots$ There were no coefficient different from 1 in front of these variables.\nThis is particularly suited to some sequences involving combinations and binomial coefficients.\nHowever, for sequences whose terms count permutations, it is more useful to consider a generating function with respect to the monomials $$ 1, x, \\frac{x^2}{2 !}, \\frac{x^3}{3 !}, \\ldots $$ Actually, we have seen these patterns in the Taylor series:\nThese monomials arise in the Taylor series $$ e^x=\\sum_{n=0}^{\\infty} \\frac{x^n}{n !}=1+x+\\frac{x^2}{2 !}+\\frac{x^3}{3 !}+\\cdots $$\nThe exponential generating function for the sequence $\\left(h_n\\right){n \\geq 0}$ is defined to be $$ g^{(e)}(x)=\\sum{n=0}^{\\infty} h_n \\frac{x^n}{n !}=h_0+h_1 x+h_2 \\frac{x^2}{2 !}+h_3 \\frac{x^3}{3 !}+\\cdots $$\nExample 1 Let $n$ be a positive integer. Determine the exponential generating function for the sequence of numbers: $$ P(n, 0), P(n, 1), P(n, 2), \\ldots, P(n, n) $$ where $P(n, k)$ denotes the number of $k$-permutations of an $n$ element set.\nSolution\nWe want to insert $P$\u0026rsquo;s into the sequence as $h$\u0026rsquo;s. $$ \\sum_{k=0}^{n} = \\frac{P(n,k)x^k}{k!} = \\sum_{k=0}^{n} \\frac{n!}{k!(n-k)!} \\cdot x! = \\sum_{k=0}^{n}\\binom{n}{k}x^k = (1+x)^n $$\nWe can see that the results becomes really clean.\nExample 2 Let $a$ be any real number. Determine the exponential generating function for the sequence $a_0, a_1, a_2, \\ldots$\nSolution $$ \\sum_{k=0}^{n} = \\frac{a^kx^k}{k!} = e^{ax} $$\nLemma\nLet $S$ be the multiset ${n_1 \\cdot a_1, n_2 \\cdot a_2, \\ldots ,n_k \\cdot a_k}$, where $n_1, n_2, \\ldots ,n_k$ are nonnegative integers.\nLet $h_n$ be the number of $n$-permutations of $S$. Then, the exponential generating function $g^{(e)}$ for the sequence $(h_n){n \\ge 0}$ is given by $$ g^{(e)}(x) = f{n_1}(x)f_{n_2}(x)\\cdots f_{n_k}(x). $$\nwhere, for $i = 1,2, \\ldots ,k$,\n$$ f_{n_i}(x) = 1 + x + \\dfrac{x^2}{2!}+ \\cdots + \\dfrac{x^{n_i}}{n_i!}. $$\nProof\nAn $n$-permutation of $S$ is a string of the form:\n$T = t_1 \\cdots t_n$, where $t_i \\in S,\\forall i \\in [n]$ and the number of $a_i$\u0026rsquo;s in $T \\leq n_i, \\forall i \\in [k]$.\nDenote: $P_n = $ number of $n$-permutation of $S$. To count $P_n$:\nChoose the multiplicities of each $a_i$ in our string: $j_i$ $a_1$'s, $j_2$ $a_2$'s, \u0026hellip;, $j_k$ $a_k$'s Restriction: $j_1 + \\ldots + j_k = n, j_i \\in [n_i], \\forall i \\in [k]$ The number of $n$-permutations with $j_i$ $a_1$'s, $j_2$ $a_2$'s, \u0026hellip;, $j_k$ $a_k$'s is: $$ \\frac{n!}{j_1! \\cdots j_k!} $$\n$$ \\begin{align*} \\implies P_n = \\sum_{(j_1, \\ldots,j_k): \\text{restriction}}\\frac{n!}{j_1! \\cdots j_k!} \\end{align*} $$ Consider the function: $$ g(x) = \\left( \\sum_{j_1 = 0}^{n_1} \\frac{x^{j_1}}{j_1} \\right) \\cdots \\left( \\sum_{j_k = 0}^{n_k} \\frac{x^{j_k}}{j_k} \\right) $$ Goal: to show that the coef of $x^n$ in $g$ is equal to $P_n / n!$ $$ \\begin{align*} g(x) \u0026amp;= \\sum_{0\\leq j_i \\leq n_i}\\frac{x^{j_1}}{j_1!} \\cdots\\frac{x^{j_k}}{j_k!}\\\\ \u0026amp;=\\sum_{n=0}^{n_1+\\ldots + n_k} \\left( \\sum_{(j_1,\\ldots,j_k):\\text{restriction}} \\frac{1}{j_1! \\cdots j_k!}\\right)\\cdot x^n \\end{align*} $$ This whole thing gives us: $$ g(x) = \\sum_{n=0}^{n_1 + \\ldots + n_k} \\frac{P_n}{n!}x^n $$\nExample 3 Let $h_n$ denote the number of $n$-digit numbers with digits 1,2, or 3 , where the number of 1 \u0026rsquo;s is even, the number of 2 \u0026rsquo;s is at least three, and the number of 3 \u0026rsquo;s is at most four. Determine the exponential generating function $g^{(e)}(x)$ for the resulting sequence of numbers $\\left(h_n\\right)_{n \\geq 0}$.\nSolution $$ \\begin{align*} g^{(e)}(x) \u0026amp;= \\left( 1 + \\frac{x^2}{2!} + \\frac{x^4}{4!} + \\ldots \\right) \u0026amp;\u0026amp; \\text{choice of num of 1's}\\\\ \u0026amp;\\cdot \\left( \\frac{x^3}{3!} + \\frac{x^4}{4!} + \\frac{x^5}{5!} + \\ldots \\right) \u0026amp;\u0026amp; \\text{choice of num of 2's}\\\\ \u0026amp;\\cdot \\left( 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!} \\right)\u0026amp;\u0026amp; \\text{choice of num of 3's} \\end{align*} $$ The expansion becomes: $$ g^{(e)}(x) = \\sum \\frac{x^{a_1}x^{a_2}x^{a_3}}{a_1!a_2!a_3!} $$\nCoefficient of $x^n$ in $g^{(e)}(x)=$ $$ \\sum_{}^{} \\frac{1}{a_1!a_2!a_3!} = \\frac{h_n}{n!} $$\nHow can we simplify this: $$ \\left( 1 + \\frac{x^2}{2!} + \\frac{x^4}{4!} + \\ldots \\right) = \\frac{e^x + e^{-x}}{2} $$\nand\n$$ \\left( \\frac{x^3}{3!} + \\frac{x^4}{4!} + \\frac{x^5}{5!} + \\ldots \\right) = e^x - 1 - x - \\frac{x^2}{2} $$\nExample 4 Determine the number of ways to color the squares of a 1-by-$n$ chessboard, using the colors, red, white, and blue. Also, an even number of squares are to be colored red.\nSolution $$ g^{(e)}(x) = e^x \\cdot e^x \\cdot \\left( \\frac{e^x + e^{-x}}{2} \\right) = \\frac{e^{3x} + e^x}{2} $$ Using a same logic.\nExample 5 Determine the number $h_n$ of $n$-digit numbers with each digit odd, where the digits 1 and 3 occur an even number of times.\nSolution\nAllowed digits $ = \\set{1, 3, 5, 7, 9}$\nNumber of 1\u0026rsquo;s and 3\u0026rsquo;s are both even.\n$$ g^{(e)}(x) = \\left( 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots \\right)^3 \\cdot \\left( 1 + \\frac{x^2}{2!} + \\frac{x^4}{4!} + \\cdots \\right)^2 $$\nWhere the first term correspondes to choices for number of 5\u0026rsquo;s,7\u0026rsquo;s, and 9\u0026rsquo;s. Since they are all the same, the term is raised to power of 3. Similarly, the second term corrspondes to choices for 1\u0026rsquo;s and 3\u0026rsquo;s.\nSimplify this we get: $$ \\begin{align*} g^{(e)}(x) \u0026amp;= \\left( \\frac{e^{-x}+e^x}{2} \\right)^2 \\cdot e^{3x}\\\\ \u0026amp;= \\frac{1}{2}e^{3x} + \\frac{e^x + e^{5x}}{4}\\\\ \u0026amp;= \\frac{1}{4} \\cdot \\left( 2e^{3x} + e^x + e^{5x} \\right) \\end{align*} $$ Now, want to find the coefficients of each term in the Taylor\u0026rsquo;s Expansion.\n$e^x = \\left( 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots \\right)$\n$e^{5x} = \\left( 1 + 5x + \\frac{(5x)^2}{2!} + \\frac{(5x)^3}{3!} + \\cdots \\right)$\n$2e^{3x} = \\left( 2 + 2\\cdot(3x) + 2\\cdot\\frac{(3x)^2}{2!} + \\cdots \\right)$\n$$ g^{(e)}(x) = \\frac{1}{4} \\cdot \\sum_{n \\geq 0}\\left( 1+5^n+2\\cdot3^n \\right) \\frac{x^n}{n!} $$ The answer of the counting problem becomes: $$ h_n = \\frac{1 + 5^n + 2 \\cdot 3^n}{4} $$\nExample 6 Determine the number $h_n$ of ways to color the squares of a 1-by-n board with colors \u0026hellip;\n==\u0026hellip;==\nSolution\nTBC\nExample 7 Example 8 This example illustrates how to use generating functions to solve linear homogeneous recurrence relations with constant coefficients.\nSolve the recurrence relation: $$ h_n = 5h_{n-1} - 6h_{n-2} \\quad (n \\geq 2) $$ with initial values $h_0 = 1, h_1 = -2$\nPreviously, we get the expression: $$ g(x) = \\frac{5}{1-2x} - \\frac{4}{1-3x} $$ Now, use the fact that $\\frac{1}{1-x} = 1 + x + x^2 + x^3 + \\ldots$, we have: $$ \\begin{align*} g(x) \u0026amp;= 5 \\sum_{n \\geq 0} (2x)^n - 4 \\sum_{n \\geq 0}(3x)^n\\\\ \u0026amp;= \\sum_{n \\geq 0}(5 \\cdot 2^n - 4 \\cdot 3^n)x^n\\\\ \u0026amp;\\implies\\\\ h_n \u0026amp;= 5 \\cdot 2^n - 4 \\cdot 3^n \\end{align*} $$ $\\blacksquare$\nTowers of Hanoi puzzle revisited Instead of using induction, what else can we do?\nLet $h_n$ be the number of moves to transfer $n$ disk from one peg to a different peg and $h_n = 2h_{n-1} + 1$ for $n \\geq 1, h_0 = 0$. Find a closed formula for $h_n$.\nSolution\nNote that this recurrence relation is non-homegeneous. (Definition below)\nNow, back to the solution. Define the following generating functions based on the expression $h_n - 2h_{n-1}-1 = 0$. $$ \\begin{align*} g_1(x) \u0026amp;= h_0 + h_1x + h_2x^2 + h_3x^3 + \\ldots \u0026amp;\u0026amp; \\text{As usual}\\\\ g_2(x) \u0026amp;= -2h_{-1} -2h_0x - 2h_1x^2 - 2h_2x^3 + \\ldots \u0026amp;\u0026amp; \\text{using } h_{-1} = 0\\\\ g_3(x) \u0026amp;= 0 -1x - 1x^2 - 1x^3 + \\ldots \u0026amp;\u0026amp; \\text{using } a_0 = 0, a_i = -1,\\forall i \\geq 1 \\end{align*} $$\nSum them up, we get a zero function: $(g_1 + g_2 + g_3)(x) = 0, \\forall x \\geq 0$\nNow, express $g_2, g_3$ in terms of $g_1$: $$ \\begin{align*} g_2(x) \u0026amp;= g(x) \\cdot (-2x)\\\\ g_3(x) \u0026amp;= -x - x^2 - x^3 - \\ldots = \\frac{-x}{1-x}\\\\ \u0026amp;\\implies\\\\ g_1(x) \u0026amp;\\cdot (1-2x) + g_3(x) = 0\\\\ \u0026amp;\\implies\\\\ g_1(x) \u0026amp;= \\frac{x}{(1-x)(1-2x)} \\end{align*} $$ We want to find Taylor series of this thing. The starting point of breaking this to parts is to construct linear terms: $$ \\begin{align*} g_1(x) \u0026amp;= \\frac{x}{(1-x)(1-2x)} = \\frac{c_1}{1-x} + \\frac{c_2}{1-2x} \\end{align*} $$ Solving for $c_1$ and $c_2$ we get: $$ g_1(x) = \\frac{-1}{1-x} + \\frac{1}{1-2x} $$ As we have $\\frac{1}{1-x} = 1 + x + x^2 + \\ldots$, we conclude: $$ \\begin{align*} g_1(x) \u0026amp;= -\\sum_{n \\geq 0} x^n + \\sum_{n \\geq 0} (2x)^n\\\\ g_1(x) \u0026amp;= \\sum_{n \\geq 0}(2^n - 1)x^n\\\\ \\implies \\\\ h_n \u0026amp;= 2^n - 1 \\end{align*} $$ Now we get THE function for Hanoi\u0026rsquo;s Tower.\nDefinition Homogeneous recurrence relation is a recurrence of the form: $$ h_n = a_1 h_{n-1} + a_2h_{n-2} + \\ldots + a_kh_{n-k} $$ Where $k$ and $a_i$\u0026rsquo;s are constants.\nDefinition The characteristic polynomial of the homogeneous linear recurrence: $$ \\begin{align*} \\frac{P(x)}{x^{n-k}} \u0026amp;= \\frac{x^n - (a_1 x^{n-1} + a_2 x^{n-2} + \\ldots + a_k x^{n-k})}{x^{n-k}}\\\\ r(x) \u0026amp;= x^k - (a_1x^{k-1} + a_2x^{k-2} + \\ldots + a_kx) \\end{align*} $$ $r(x)$ is the characteristic polynomial of the homogeneous linear recurrence.\nTheorem Let $(h_i)_{i\\ge0}$ be a sequence of numbers defined by the homogeneous recurrence relation\n$$ h_n + a_1h_{n-1}+\\cdots+a_kh_{n-k}=0, , (n \\ge k) $$\nof order $k$ and with initial values for $h_0, h_1,\\ldots,h_{k-1}$.\nLet $r(x) = x^k +a_1 x^{k-1}+a_2x^{k-2}+\\cdots+a_k$ be the characteristic polynomial of this recurrence relation.\nThen, the generating function of $(h_i)_{i\\ge0}$ is given by $g(x) = p(x)/q(x)$, where $q(x) = x^k r(1/x)$ and\n$$ p(x) = h_0 + (h_1+a_1h_0)x+(h_2+a_1h_1+a_2h_0)x^2+\\cdots+(h_{k-1}+a_1h_{k-2}+\\cdots+a_{k-1}h_0)x^{k-1} $$\n$p(x)$ in another form:\n$$ p(x) = \\sum_{i = 0}^{k - 1}\\left( \\sum_{j = 0}^{i} a_jh_{i-j}\\right)x^i $$\nwhere we define $a_0 = 1$.\nNote that this is slightly different from the previous definition, but this is still a homogeneous recurrence relations.\nProof is not given\u0026hellip; I think just understanding this theorem\u0026rsquo;s statement what matters.\n","permalink":"https://ohuro.me/posts/413_generating-functions/","summary":"Generating Functions Generating functions can be regarded as algebraic objects whose formal manipulation allows us to count the number of possibilities for a problem by means of algebra.\nDefinition\nLet $h_0, h_1, h_2, h_3, \\ldots$ be an infinite sequence of numbers. Its generating function is defined to be the infinite series: $$ g(x) = h_0 + h_1x + h_2x^2 + \\cdots + h_nx^n + \\cdots $$\nExample 1 The generating function of the infinite sequence $1, 1, 1, \\ldots$ is: $$ g(x) = 1 + x + x^2 + x^3 + \\cdots $$ for $|x| \u0026lt; 1$.","title":"413_generating Functions"},{"content":"World Coordinates and Image coordinates Pinhole Camera Model $$ x = K \\left[ \\begin{array}{ll} R \u0026amp; t \\end{array} \\right] x $$\nx: Image Coordinates: $(u, v, 1)$ K: Intrinsic Matrix $(3 \\times 3)$ R: Rotation $(3 \\times 3)$ t: Translation $(3 \\times 1)$ X: World Coordinates: $(X, Y, Z, 1)$\nBasically, from the right side to the leftside, it is transforming a point (1) from the world coordinates to camera coordinates, and then project the point (2) from camera coordinates down to the image plane.\n(1) is done by the $\\left[\\begin{array}{ll} R \u0026amp; t \\end{array}\\right]$, the extrinsic matrix (rotation and translation).\n(2) is done by K, the intrinsic/projection matrix.\nThe Intrinsic Matrix How we do the projection?\nFirst, let\u0026rsquo;s look at this projection from camera coordinate to image plane.\nAn important projection diagram to remember(CS445 Hoiem) In a matrix form, if we set the camera center at $(0, 0, 0)$, this is actually easy to express: $$ K =\\left[\\begin{array}{ccc} f \u0026amp; 0 \u0026amp; u_0 \\\\ 0 \u0026amp; f \u0026amp; v_0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{array}\\right] $$\nNow if we just use the default R and t, the model is: $$ w\\left[\\begin{array}{l} u \\\\ v \\\\ 1 \\end{array}\\right] = \\left[\\begin{array}{cccc} f \u0026amp; 0 \u0026amp; u_0 \u0026amp; 0\\\\ 0 \u0026amp; f \u0026amp; v_0 \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{array}\\right] \\left[\\begin{array}{c} X \\\\ Y \\\\ Z \\\\ 1 \\end{array}\\right] $$ Do some calculation, then you find out that this is exactly the same as the diagram above.\nThe Extrinsic Matrix $$ \\left[\\begin{array}{ll} R \u0026amp; t \\end{array}\\right] = \\left[\\begin{array}{cccc} r_{11} \u0026amp; r_{12} \u0026amp; r_{13} \u0026amp; t_x \\\\ r_{21} \u0026amp; r_{22} \u0026amp; r_{23} \u0026amp; t_y \\\\ r_{31} \u0026amp; r_{32} \u0026amp; r_{33} \u0026amp; t_z \\end{array}\\right] $$\nThe R here is the rotation matrix for 3D coordinate. World coordinate $\\xrightarrow{\\text{rotate to }}$ Camera coordinate direction. This R is also orthonormal.\nThe t here traslates the world origin to the camera center.\nIf we know that position $(x, y, z)$ of the camera in the world coordinate, we can also set up this simple equation for calculating R and t. $$ X_c =\\left[\\begin{array}{ll} R \u0026amp; t \\end{array}\\right]\\left[\\begin{array}{c} X_w \\\\ 1 \\end{array}\\right]=RX_w+t=0 \\\\ $$\nThe Complete Model $$ x=\\mathbf{K}\\left[\\begin{array}{ll} \\mathbf{R} \u0026amp; \\mathbf{t} \\end{array}\\right] x \\Rightarrow w\\left[\\begin{array}{l} u \\\\ v \\\\ 1 \\end{array}\\right] =\\left[\\begin{array}{ccc} f \u0026amp; 0 \u0026amp; u_0 \\\\ 0 \u0026amp; f \u0026amp; v_0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{array}\\right] \\left[\\begin{array}{cccc} r_{11} \u0026amp; r_{12} \u0026amp; r_{13} \u0026amp; t_x \\\\ r_{21} \u0026amp; r_{22} \u0026amp; r_{23} \u0026amp; t_y \\\\ r_{31} \u0026amp; r_{32} \u0026amp; r_{33} \u0026amp; t_z \\end{array}\\right] \\left[\\begin{array}{c} X \\\\ Y \\\\ Z \\\\ 1 \\end{array}\\right] $$\nExercise Take home question 1:\nSuppose the camera axis is in the direction of $(x=0, y=0, z=1)$ in its own coordinate system. What is the camera axis in world coordinates given the extrinsic parameters $R, t$ ?\nSolution\nNote that this is not asking the projection on 2D image, but it is asking: after the rotation what is the direction of this camera?\nLet $X_c$ denote the direction of this camera in its own coordinate. So $X_c=\\left[\\begin{array}{l}0 \\ 0 \\ 1\\end{array}\\right]$ Let $X_w$ denote the direction of this camera in world coordinate. So $\\quad X_w=\\left[\\begin{array}{l}x_w \\ y_w \\ z_w\\end{array}\\right]$ We have equation:\n\\begin{align*} X_c \u0026amp; =\\left[ \\begin{array}{ll} R \u0026amp; t \\end{array} \\right] \\left[ \\begin{array}{c} X_w \\\\ 1 \\end{array} \\right] \\\\ X_c \u0026amp; =R X_w+t \\\\ R X_w \u0026amp; =X_c-t \\\\ X_w \u0026amp; =R^{-1}\\left(X_c-t\\right) \\\\ \u0026amp; =R^{-1} X_c-R^{-1} t\\\\ \u0026amp; =R^{T} X_c-R^{T} t \\quad \\text { as } R^{-1}=R^{T} \\\\ \\end{align*}\nWhen we consider just the direction, translation doesn\u0026rsquo;t matter. So: $$ X_w=R^T X_c $$\nExercise Take home question 2:\nSuppose a camera at height $y=h,(x=0, z=0)$ observes a point at $(u, v)$ known to be on the ground $(y=0)$. Assume $R$ is identity. What is the 3D position of the point in terms of $f, u_0, v_0$ ?\nSolution:\nThe camera is at $(x, y, z) = (0, h, 0)$. And since we know that the rotation matrix R is an identity matrix, we can set up an equation to solve for t:\n\\begin{align*} X_{cc} \u0026amp; =\\left[\\begin{array}{ll} I \u0026amp; t \\end{array}\\right]\\left[\\begin{array}{c} X_{cw} \\\\ 1 \\end{array}\\right]=X_{cw}+t=0 \\\\ \\Rightarrow t \u0026amp; =-X_{cw} = \\left[ \\begin{array}{c} 0\\\\ -h\\\\ 0\\\\ \\end{array} \\right] \\end{align*}\nWhere $X_{cc}$ stands for the position of camera at camera coordinate. $X_{cw}$ stands for the position of camera at world coordinate.\nThen, we know that in the camera coordinate, the point is observed at $(u, v)$. This fills in the left side of the Pinhole Camera Model.\nWe want to solve for the position $X_{pw}$ of the point in the world coordinate. We also know that this point is on the ground, so its $y$ value is 0. As we have all the information we want, writing out the formula of the Pinhole Camera Model and we should be able to solve the problem:\n$$ w\\left[\\begin{array}{l} u \\\\ v \\\\ 1 \\end{array}\\right]=\\left[\\begin{array}{lll} f \u0026amp; 0 \u0026amp; u_0 \\\\ 0 \u0026amp; f \u0026amp; v_0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{array}\\right]\\left[\\begin{array}{cccc} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; -h \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{array}\\right]\\left[\\begin{array}{c} x_{p m} \\\\ 0_{p m} \\\\ z_{p m} \\\\ 1 \\end{array}\\right] $$\nReference CS445 Derek Hoiem: https://courses.engr.illinois.edu/cs445/fa2023/\n","permalink":"https://ohuro.me/posts/445_camera/","summary":"World Coordinates and Image coordinates Pinhole Camera Model $$ x = K \\left[ \\begin{array}{ll} R \u0026amp; t \\end{array} \\right] x $$\nx: Image Coordinates: $(u, v, 1)$ K: Intrinsic Matrix $(3 \\times 3)$ R: Rotation $(3 \\times 3)$ t: Translation $(3 \\times 1)$ X: World Coordinates: $(X, Y, Z, 1)$\nBasically, from the right side to the leftside, it is transforming a point (1) from the world coordinates to camera coordinates, and then project the point (2) from camera coordinates down to the image plane.","title":"445_camera"},{"content":"Pasting Images Method 1 — Cut and paste the images. Feathering\nGives a smoother transition. But that\u0026rsquo;s all\u0026hellip; Alpha composting\nOutput = foreground $\\times$ mask + background $\\times$ (1 $-$ mask). We can also use alpha compositing together with the feathering — simply bluring the mask will give us a good feathering. This method is also good for multilayer processing, which allwos the compositing to be more complicated. Method 2 — Pyramid Blending At low frequencies, blend slowly At high frequencies, blend quickly Burt and Adelson 1983 Implementation: Build Laplacian pyramids for each image Build a Gaussian pyramid of region mask Blend each level of pyramid using region mask from the same level $$ L_{12}^i=L_1^i \\cdot R^i+L_2^i \\cdot\\left(1-R^i\\right) $$\nCollapse the pyramid to get the final blended image Method 3: Poisson Blending A good blend should preserve gradient of source region withough changing the background\nTreat pixels as variables to be solved\nMinimize squared difference between gradients of foreground region and gradients of target region Keep background pixels constant $$ \\mathbf{v}=\\underset{\\mathbf{v}}{\\operatorname{argmin}} \\sum_{i \\in S, j \\in N_i \\cap S}\\left(\\left(v_i-v_j\\right)-\\left(s_i-s_j\\right)\\right)^2+\\sum_{i \\in S, j \\in N_i \\cap \\neg S}\\left(\\left(v_i-t_j\\right)-\\left(s_i-s_j\\right)\\right)^2 $$\nWe want to put source image on the background image. The result is called \u0026ldquo;target image\u0026rdquo;. So, inside the pasted regeion, we want to make the gradient similar to the source image. But in the edge of our pasted regeion, we also want to make the transformation smaller. We can then use a least square method to solve for a vector $\\vec v$ to minimize the error.\nIn this example here, we have:\n$v_3 - v_1 \\approx s_{10} - s_{6} $\n$v_1 - t_2 \\approx s_6 - s_2$\nNote that this method doesn\u0026rsquo;t care color, so putting a bear in a pool of water will make the bear turn green.\nMethod 4: Blending with Mixed Gradients Use fourground or background gradient with larger magnitude as the guiding gradient. Since the larger gradient is always percieved, it is more likely to have the result of \u0026ldquo;writing something on the wall\u0026rdquo; instead of \u0026ldquo;sticking an printed image on the wall\u0026rdquo;.\nReference CS445 Derek Hoiem: https://courses.engr.illinois.edu/cs445/fa2023/\n","permalink":"https://ohuro.me/posts/445_blending-warping/","summary":"Pasting Images Method 1 — Cut and paste the images. Feathering\nGives a smoother transition. But that\u0026rsquo;s all\u0026hellip; Alpha composting\nOutput = foreground $\\times$ mask + background $\\times$ (1 $-$ mask). We can also use alpha compositing together with the feathering — simply bluring the mask will give us a good feathering. This method is also good for multilayer processing, which allwos the compositing to be more complicated. Method 2 — Pyramid Blending At low frequencies, blend slowly At high frequencies, blend quickly Burt and Adelson 1983 Implementation: Build Laplacian pyramids for each image Build a Gaussian pyramid of region mask Blend each level of pyramid using region mask from the same level $$ L_{12}^i=L_1^i \\cdot R^i+L_2^i \\cdot\\left(1-R^i\\right) $$","title":"445_Blending"},{"content":"Texture Synthesis and Hole-Filling How do we cut something out of an image, and fill the hole naturally?\nDefinition Texture depicts spacially repeating patterns.\nTexture Synthesis Create new samples of a given texture. Many applications: virtual environments, hole-filling, texturing surfaces.\nThe challenge:\nNeed to model the whole spectrum: from repeated to stochastic texture.\nOne idea:\nCompute statistics of input texture Generate a new texture that keeps those same statistics. But it is hard to model those probabilities distributions.\nAnother idea: ==Efros \u0026amp; Leung algorithm==\nGiven the neighbour, we want to calculate the probabiliy of appearing this pixel $p$ in the middle.\nHow to match pixels?\nGaussian-weighted SSD (sum square difference) gives us more emphasis on nearby pixels. \\begin{align*} \\text{SSD}(P, Q) = \\sum_{i, j}(p_{ij} - q_{ij})^2 w_{ij}\\\\ \\text{where } w_{ij} = e^{ \\frac{-(1-w/2)^2 - (j - h/2)^2}{2 \\sigma^2} } \\end{align*}\nwhere $P$, and $Q$, are two patches, $w$ and $h$ in the $w_{ij}$ is the width and height of the patch.\nWhat order to fill in new pixels?\n\u0026ldquo;Onion skin\u0026rdquo; order: pixels with most neighbors are synthesized first. How big should the patches be? The size of neighborhood window decides how stochasticity of the texture. A smaller window size gives a more random output.\nTexture synthesis algorithm While image not filled:\nGet unfilled pixels with filled neighbors, sorted by the numebr of filled neighbors. (priority queue?) For each pixel, get top N matches based on visible neighbors. This is where we use the Gaussian-weighted SSD. Randomly select one of the matches and copy pixels from it. This algorithm can be used for hole filling, extrapolation, \u0026hellip;\nHole-Filling We can just use the texture synthesis algorithm.BUT, the order of filling matters.\nSometimes, we can add more weights for the continuous edges when peforming the onion filling. (Gradient sensitive). It makes the edges to be more intact.\nWe want it to be faster The Efros \u0026amp; Leung texture synthesis algorithm is simple and good, but too slow\u0026hellip;\nThe next iteration is: Image quilting (Efros \u0026amp; Freeman 2001).\nIt depends on the observation that: neighbor pixels are highly correlated. Now, instead of filling pixel by pixel, we fill block by block.\nWe need to put the tiles together. To make them look seemless, we can use this minimal error boundary cut. We calculate the square difference of the overlapping part, and calcualte the boundary. Using this simplified Dijikstra\u0026rsquo;s algorithm, we can easy calculate what we want.\nTexture Transfer Very similar to texture synthesis, but in a more restricted condition: $$ \\text{cost} = \\alpha * \\text{SSD}{overlap} + (1-\\alpha)* \\text{SSD}{transfer} $$\nImage Analogies Intelligent Scissors and Graph Cuts We want to find seams and boundaries. Then, CUT!\nFundamental Concepts: The image as a Graph\nIntelligent Scissers: Good boundary = short path Graph cuts: Good region has low cutting cost. The goal ofSemi-automated segmentation: User provides imprecise and incomplete specification of region — your algorithm has to read his/her mind.\nA good region:\nContains small range of color/ texture. Looks different than background Compact A good boundary:\nHigh gradient along boundary Gradient in right direction Smooth CS445 Hoiem. Good Region and good boundary. Intelligent Scissors Consider these image as a graph. Think of pixels as nodes, and think of cost of the path between two pixels as weighted edges.\nIntelligent Scissors. Mortenson and Barrett (SIGGRAPH 1995) Using this model, a **good boundary** of an image has a short path through the graph. Formulation: find good boundary between seed points.\nChallenges:\nMinimize interaction time Define what makes a good boundary Efficienctly find it Luckily, these can be achieved by using Dijkstra\u0026rsquo;s algorithm with a properly defined graph.\nIntelligent Scissors method Define boundary cost between neighboring pixels\nLower if edge is present\nLower is gradient is strong\nLower if gradient is in direction of boundary\nUser specifies a starting point (seed)\nA little snapping to the edges makes pixel-perfect operation not neccessary. Compute lowest cost from seed to each other pixel\nDijkstra\u0026rsquo;s shortest path algorithm Get path from seed to cursor, choose new seed, repeat\nUsing this boundary cost, the path tends to follow the edges of objects. It makes the algorithm works. Dijkstra\u0026rsquo;s algorithm makes it fast.\nGraph Cuts CS445 Hoiem Look at the energe function, $y$ in the function represents the label of pixels (whether they belong to label 0 or 1). And, there are two kinds of cost.\nThe first one is the unary cost, it measures whether the pixel is more similar to label 0 or 1. The second one is a pair wise cose, which allows groups of pixels to be assigned to the same label. The goal of the graph cut algorithm is to find a cut through the graph to seperate the foreground and the background.\nThe Graph Cut Segmentation algorithm: Define graph usually 4-connected or 8-connected Set weights to foreground/background Color histogram or mixture of Gaussians for background and foreground. More foreground tends to be negative. $$ \\textit { unary_ potential }(x)=-\\log \\left(\\frac{P\\left(c(x) ; \\theta_{\\text {foreground }}\\right)}{P\\left(c(x) ; \\theta_{\\text {background }}\\right)}\\right) $$ 3. Set weights for edges between pixels\n$$ \\textit { edge_ potential }(x, y)=k_1+k_2 \\exp\\left{ \\frac{-|c(x)-c(y)|^2}{2 \\sigma^2} \\right} $$\nApply min-cut/max-flow algorithm foreground, background models Return to 2, using current labels to compute foreground, background models. Limitations of Graph Cut Requires associative graphs Connected notes should prefer to have the same label Is optimal only for binary problems Reference CS445 Derek Hoiem: https://courses.engr.illinois.edu/cs445/fa2023/\n","permalink":"https://ohuro.me/posts/445_synthesizing-cutting/","summary":"Texture Synthesis and Hole-Filling How do we cut something out of an image, and fill the hole naturally?\nDefinition Texture depicts spacially repeating patterns.\nTexture Synthesis Create new samples of a given texture. Many applications: virtual environments, hole-filling, texturing surfaces.\nThe challenge:\nNeed to model the whole spectrum: from repeated to stochastic texture.\nOne idea:\nCompute statistics of input texture Generate a new texture that keeps those same statistics. But it is hard to model those probabilities distributions.","title":"445_Synthesizing and Cutting"},{"content":"How do you view the world Cones: cone-shaped, less sensitive, operate in high light, color vision\nRods: rod-shaped, highly sensitive, operate at night, gray-scale vision, slower to respond\nObservation In a clear night, there are more stars off-center. This is because you have more rods in the middle, while more cones elsewhere.\nHow to express colors? Basically, the most intuitive expression is the RGB color space. But it is not a linear color space. We perfer expressing using CIE-XYZ color space, which makes the calculation much easier.\nHow is light reflected from a surface? Lambertian surface Some light is absorbed The rests are reflected in all direction (diffuse reflection. Think of how you view a piece of white paper) Diffuce reflection, the intensity does depend on illumination angle. $$ I(x) = \\rho(x)(S\\cdot N(x)) $$\nwhere $\\rho$ is albedo, $S$ is the directional source, $N$ is the surface normal, $I$ is the image complexity.\nHowever, perceived intensity does not depend on viewer angle\nSpecular Reflection, consider mirror.\nMost surfaces have these two kinds of reflection. We also have more complicated reflections and refractions.\nColor spaces CIE-XYZ As above.\nCIE L*a*b* This color space sepearates the luminance and chrominance. It turns out that, people gains more information from the luminance channel.\nHSV Hue, Saturation, Value\nYCbCr This is good for fast computation and easier compression.\nEnhancement Color balancing: White balance Simple idea: multiply R, G, and B values by separate constants. $$ \\left[\\begin{array}{l} \\tilde{r} \\ \\tilde{g} \\ \\tilde{b} \\end{array}\\right]=\\left[\\begin{array}{ccc} \\alpha_r \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; \\alpha_g \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; \\alpha_b \\end{array}\\right]\\left[\\begin{array}{l} r \\ g \\ b \\end{array}\\right] $$\nGray world assumption. Calculate:\nred_avg as the mean of all the red channel\navg = (red_avg + green_avg + blue_avg) / 3\nChoose $\\alpha_r$ by calculating avg/red_avg\nUse a natural color(white, gray) regeion for calibration. https://stackoverflow.com/questions/54470148/white-balance-a-photo-from-a-known-point gave me an idea:\nGiven an natrual color point, calculate:\nThe luminance of the given point by (choice_r + choice_g + choice_b) / 3 Calculate $\\alpha_r$ by choice_lum/choice_r Gamma adjustment $$ i_{\\text {out }}=i_{\\text {in }}^\\gamma $$\nNote that the input range should be in 0 to 1.\nHistorgram equalization Basic idea: reassign values so that the number of pixels with each value is more evenly distributed. So the histogram should look much smoother instead of having high hills and low valleys.\nThere is an algorithm for global histogram equalization.\nGoal: Given image with pixel values $0 \\leq p j \\leq 255, j=0 . . N$ specify function $\\mathrm{f}(i)$ that remaps pixel values, so that the new values are more broadly distributed\nCompute cumulative histogram: $c(i), i=0 . .255$ $$ h(i)=\\sum_{j \\in \\text { pixels }} \\mathbf{1}\\left(p_j==i\\right), c(i)=c(i-1)+h(i) $$ $\\mathrm{f}(i)=\\alpha \\cdot \\frac{c(i)}{N} \\cdot 255+(1-\\alpha) \\cdot i$ Blends between original image and image with equalized histogram Reference CS445 Derek Hoiem: https://courses.engr.illinois.edu/cs445/fa2023/\n","permalink":"https://ohuro.me/posts/445_color_basics/","summary":"How do you view the world Cones: cone-shaped, less sensitive, operate in high light, color vision\nRods: rod-shaped, highly sensitive, operate at night, gray-scale vision, slower to respond\nObservation In a clear night, there are more stars off-center. This is because you have more rods in the middle, while more cones elsewhere.\nHow to express colors? Basically, the most intuitive expression is the RGB color space. But it is not a linear color space.","title":"445_color_basics"},{"content":"Definition A multinomial coefficient is:\n$$ \\binom{n}{n_1 n_2 \\cdots n_t} =\\frac{n !}{n_{1} ! n_{2} ! \\cdots n_{t} !} $$\nHere, $n_1, n_2, \\ldots, n_t$ are nonnegative integers with $$ n_1+n_2+\\cdots+n_t=n $$\nPascal\u0026rsquo;s theorem for multinomial coefficients Pascal\u0026rsquo;s theorem for Let $n, k \\in \\mathbb{N}$ and $(n)_{i=1}^k$ be natural numbers so that $$ n_1+\\cdots+n_k=n $$ Then,\n$$ \\binom{n}{n_1, n_2, \\ldots, n_k} = \\binom{n-1}{n_1-1, n_2, \\ldots, n_k} + \\ldots + \\binom{n-1}{n_1, n_2, \\ldots, n_k-1} $$\nProof\nCombinatorial argument: $$ \\text { Let } S={\\underbrace{1, \\ldots, 1}{n_1}, \\underbrace{2, \\ldots, 2}{n_2}, \\ldots, \\underbrace{k, \\ldots, k}_{n_k}} $$ Count $N = $# of permutations of $S$.\nFor each $i \\in [k]$, define $N_i = $ # of permutations of $S$ where the first element is equal to $i$.\nThen, $N = N_1 + N_2 + \\ldots + N_k$. Where: $$ N_i = \\frac{(n-1)!}{n_1!n_2! \\cdots n_{i-1}!(n_i-1)!n_{i+1}\\cdots n_k!} $$ Which is equal to the right side. Left side is trivial. Done.\nThe multinomial theorem Let $n$ be a positive integer. For all $x_1,\\ldots,x_t$, we have\n$$ (x_1+\\cdots+x_t)^n = \\sum \\binom{n}{n_1,, n_2, \\ldots, , n_k} x_1^{n_1}x_2^{n_2}\\cdots x_t^{n_t} $$\nwhere the summation extends over all nonnegative integral solutions $n_1,\\ldots,n_t$ of $n_1 + \\cdots + n_t = n$.\nProof\nDefine $S = {n\\cdot x_1, n\\cdot x_2 \\cdots n\\cdot x_k}$ ($n$ copies of $x_i$)\nDefine $\\mathcal{P} = $ the collection of all $n$-permutaions of the multiset $S$. $$ (x_1 + \\ldots + x_k)^n = \\sum_{a_1a_2\\ldots a_n \\in \\mathcal{P}} a_1 \\cdot a_2 \\cdots a_n $$ Imagine that you just expand this huge multiplication on the left side, then choose one $x_i$ from each parenthesis.\nQuestion: What is the number of $n$-permutation of $S$ with $n_1$ $x_1$\u0026rsquo;s, $n_2$ $x_2$\u0026rsquo;s, \u0026hellip; $n_k$ $x_k$\u0026rsquo;s, where $n_1 + n_2 + \\ldots + n_k = n$.\nAnswer: $\\binom{n}{n_1, n_2, \\ldots, n_k}$\nThen we have: $$ (x_1 + \\ldots + x_k)^n = \\sum_{a_1a_2\\ldots a_n \\in \\mathcal{P}} a_1 \\cdot a_2 \\cdots a_n = \\text{right side of the theorem} $$\n","permalink":"https://ohuro.me/posts/413_multinomial/","summary":"Definition A multinomial coefficient is:\n$$ \\binom{n}{n_1 n_2 \\cdots n_t} =\\frac{n !}{n_{1} ! n_{2} ! \\cdots n_{t} !} $$\nHere, $n_1, n_2, \\ldots, n_t$ are nonnegative integers with $$ n_1+n_2+\\cdots+n_t=n $$\nPascal\u0026rsquo;s theorem for multinomial coefficients Pascal\u0026rsquo;s theorem for Let $n, k \\in \\mathbb{N}$ and $(n)_{i=1}^k$ be natural numbers so that $$ n_1+\\cdots+n_k=n $$ Then,\n$$ \\binom{n}{n_1, n_2, \\ldots, n_k} = \\binom{n-1}{n_1-1, n_2, \\ldots, n_k} + \\ldots + \\binom{n-1}{n_1, n_2, \\ldots, n_k-1} $$","title":"413_Multinomial Coefficient"},{"content":"Unimodality of Binomial Coefficients and Sperner\u0026rsquo;s Theorem If we examine the binomial coefficients in a row of Pascal\u0026rsquo;s triangle, we notice that the numbers increase for a while and then decrease. A sequence of numbers with this property is called unimodal.\nTheorem Let $n$ be a positive integer. The sequence of binomial coefficients $$ {{n} \\choose {0}}, {{n} \\choose {1}}, \\ldots, {{n} \\choose {n}} $$ is a unimodal sequence. Mor precisely, $$ {{n} \\choose {0}} \u0026lt; {{n} \\choose {1}} \u0026lt; \\ldots \u0026lt; {{n} \\choose {\\lfloor n/2\\rfloor}} $$ and $$ {{n} \\choose {\\lceil n/2 \\rceil}} \u0026gt; \\ldots {{n} \\choose {n-1}} \u0026gt; {{n} \\choose {n}} $$\nProof\n$k \\in \\set{0, \\ldots, n}$\nCompare ${{n} \\choose {k}}$ to ${{n} \\choose {k-1}}$ by using division: $$ \\frac{{{n} \\choose {k}}}{{{n} \\choose {k-1}}} = \\frac{n-k+1}{k} \u0026lt; 1 \\iff n-k+1 \u0026lt; k \\iff \\frac{n+1}{2} \u0026lt; k $$ When $n$ is odd: $\\frac{n+1}{2}$ gives us $\\lceil \\frac{n}{2} \\rceil$\n$\\implies$ ${{n} \\choose {k}} \u0026lt; {{n} \\choose {k-1}} \\iff k \u0026gt; {\\lceil n / 2\\rceil}$\nand also ${{n} \\choose {k}} \u0026gt; {{n} \\choose {k-1}} \\iff k \u0026lt; {\\lceil n / 2\\rceil}$\nNote that: ${{n} \\choose {k}} = {{} \\choose {}}$\n==TBC==\nWe want to use this theorem to prove Sperner\u0026rsquo;s theorem. But we need some definition first.\nDefinition (Chain)\nWe say that a collection $\\mathcal{C}$ of subsets of $S$ is a chain provided that for that each pair of subsets in $C$, one is contained in the other:\n$$ A_1, A_2 \\in \\mathcal{C}, A_1 \\neq A_2, \\text{ implies that } A_1 \\subset A_2 \\text{ or } A_2 \\subset A_1. $$\nDefinition (antichain)\nAn antichain of a set $S$ is a collection $\\mathcal{A}$ of subsets of $S$ with the property that no subset in $\\mathcal{A}$ is contained in another.\nLet $\\mathcal{F}$ be a collection of sets. It is an antichain if $A, B \\in \\mathcal{F}, A\\neq B$ then $A \\nsubseteq B$ and $B \\nsubseteq A$.\nExample: $\\set{{1,2}, {2,3}, {3,4}}$\nQuestion: Let $S$ be a set of $n$ elements. What is the maximum size of a antichain on $S$?\nSperner\u0026rsquo;s theorem Let $S$ be a set of $n$ elements. The maximum size of an antichain on $S$ is $\\binom{n}{\\lfloor n/2\\rfloor}$\nProof\n$[n] = \\set{1, 2, \\ldots, n}$\nFix $\\mathcal{F}$ antichain in $[n]$\n$\\mathcal{C} = \\set{A \\subset S : |A| = {\\lfloor n/2\\rfloor}}$\n$\\mathcal{P} = \\set{(\\mathcal{C}, A): A \\in \\mathcal{F} \\cap \\mathcal{C} \\text{ and } \\mathcal{C} \\text{ is the maximal chain in }S}$. 🧐 This is CRITICAL\nWe want to proof this theorem by double conting this $\\mathcal{P}$. Our goal is to bound $|\\mathcal{F}|$ using $\\mathcal{P}$.\nThe upper bound\nQuestion 1: What is the number of maximum chain in $[n]$? Ans: n!\nQuestion 2: Given a maximal chain $\\mathcal{C}$, $|\\mathcal{F} \\cap \\mathcal{C}| \\leq ?$ Ans: 1\nBy Q1 and Q2, we can give an upper bound $|\\mathcal{P}| \\leq n!$.\nQuestion 3: Fix a set $A \\in \\mathcal{F}$. What is the number of maximal chain $\\mathcal{C}$ have $A \\in \\mathcal{C}$?\nAnswer: $|A|!$ to permutate the first half. $(n-|A|)!$ to permutate the second half. So we have $|A|! \\cdot (n-|A|)!$ such maximal chains.\nBy Q1, Q2, Q3, we have \\begin{align*} \\sum_{A \\in \\mathcal{F}}|A|! \\cdot (n-|A|)! = |\\mathcal{P}| \\leq n!\\\\ \\implies \\sum_{A \\in \\mathcal{F}}\\frac{1}{\\binom{n}{|A|}} \\leq 1 \u0026amp;\u0026amp; \\text{divide both side by }n! \\end{align*} As we have the unimodel theorem above, \\begin{align*} \\binom{n}{\\lfloor n/2 \\rfloor} \u0026amp;\\geq \\binom{n}{|A|}, \\forall A\\\\ \\frac{1}{\\binom{n}{\\lfloor n/2 \\rfloor}} \u0026amp;\\leq \\frac{1}{\\binom{n}{|A|}} \\end{align*} Then we continue: \\begin{align*} \\frac{|\\mathcal{F}|}{\\binom{n}{\\lfloor n/2 \\rfloor}} \u0026amp;\\leq \\sum_{A \\in \\mathcal{F}}\\frac{1}{\\binom{n}{|A|}} \\leq 1\\\\ |\\mathcal{F}| \u0026amp;\\leq \\binom{n}{\\lfloor n/2 \\rfloor} \\end{align*} $\\blacksquare$\nHere is an interesting application.\nLittlewood\u0026ndash;Offord theorem Let $v_1, \\ldots, v_n$ be positive integers. Let $x_1, \\ldots, x_n$ be random variables so that $\\mathbb{P}\\left(x_i=1\\right)=\\mathbb{P}\\left(x_i=-1\\right)=1 / 2$. Then, $$ \\max _r \\mathbb{P}\\left(x_1 v_1+\\cdots+x_n v_n=r\\right)=O\\left(\\frac{1}{\\sqrt{n}}\\right) \\text {. } $$ Think of this as an one dimensional random walk problem:\nProof\nLet $r \\in \\mathbb{Z}$ fixed.\nDefine: $$ \\mathcal{S} = \\set{(a_1, \\ldots, a_n): a_i \\in {±1}, \\sum_{i=1}^{n}a_iv_i = r} $$\n$$ \\mathbb{P}(x_1v_1 + \\ldots + x_nv_n = r) = \\frac{|\\mathcal{S}|}{2^n} $$\nFor each solution $s = (a_1, \\ldots, a_n) \\in \\mathcal{S}$ define: $$ A_s = \\set{i : a_i = 1} $$ Observation given this $A_s$, we can always find an $s$.\nClaim: the family $\\mathcal{F} = \\set{A_s : s \\in \\mathcal{S}}$ is an antichain.\nShort proof by contradiction: Suppose there exists some $s_1, s_2$ such that $A_{s_1} \\subset A_{s_2}$. We have $s_1$ as one solution, and it has $|A_{s_1}|$ many $+1$ steps. Also because that $A_{s_1} \\subset A_{s_2}$ , $s_2$ has all the $+1$ steps as $s_1$, but it also flips some $-1$ steps of $s_1$ to $+1$ steps. This makes the resulting point bypass the goal, $r$, which results in an contradiction as $s_2$ cannot be a solution.\nThen, by the observation, $\\exists$ a bijection between $\\mathcal{F}$ and $\\mathcal{S}$, which means $|\\mathcal{F}| = |\\mathcal{S}|$. This gives us: $$ \\mathbb{P}(x_1v_1 + \\ldots + x_nv_n = r) = \\frac{|\\mathcal{S}|}{2^n} = \\frac{|\\mathcal{F}|}{2^n} \\leq \\frac{\\binom{n}{\\lfloor n/2\\rfloor}}{2^n} $$ Further simplification: $$ \\binom{n}{\\lfloor n/2\\rfloor} \\sim \\Theta\\left(\\frac{2^n}{\\sqrt n}\\right) $$ Cancelling the $2^n$ gives us the statement of the theorem. $\\blacksquare$\n","permalink":"https://ohuro.me/posts/413_unimodality/","summary":"Unimodality of Binomial Coefficients and Sperner\u0026rsquo;s Theorem If we examine the binomial coefficients in a row of Pascal\u0026rsquo;s triangle, we notice that the numbers increase for a while and then decrease. A sequence of numbers with this property is called unimodal.\nTheorem Let $n$ be a positive integer. The sequence of binomial coefficients $$ {{n} \\choose {0}}, {{n} \\choose {1}}, \\ldots, {{n} \\choose {n}} $$ is a unimodal sequence. Mor precisely, $$ {{n} \\choose {0}} \u0026lt; {{n} \\choose {1}} \u0026lt; \\ldots \u0026lt; {{n} \\choose {\\lfloor n/2\\rfloor}} $$ and $$ {{n} \\choose {\\lceil n/2 \\rceil}} \u0026gt; \\ldots {{n} \\choose {n-1}} \u0026gt; {{n} \\choose {n}} $$","title":"413_Unimodality of binomial coef \u0026 Sperner's theorem"},{"content":"Theorem $$ \\text{For every } k \\in \\mathbb{N}, \\text{we have } R(k)\\ge 2^{k/2} $$\nBefore we start the proof, let\u0026rsquo;s look at 3 ingredients we need:\nThe probabilistic method The union bound The binomial estimate The probabilistic method Let $\\Omega$ be the set of all possible outcomes of an experiment. Suppose that each outcome is equally likely. Let $A \\subseteq \\Omega$ be an event. $$ \\text{If } \\operatorname{Prob}(A) \u0026lt; 1 \\text{, then } \\operatorname{Prob}(A^c) \u0026gt; 0 $$\nUnion Bound Let $\\Omega$ be the set of all possible outcome of an experiment. Suppose that each outcome is equally likely. Let $A_1, \\ldots, A_t \\subseteq \\Omega$ be some events. Then: $$ \\operatorname{Prob}(A_1 \\cup A_2 \\cup \\cdots A_t) \\leq \\sum_{i=1}^{t}\\operatorname{Prob}(A_i) $$ Proof\nConsider the sets: \\begin{align*} B_1 \u0026amp;= A_1\\\\ B_2 \u0026amp;= A_2 \\setminus A_1\\\\ B_3 \u0026amp;= A_3 \\setminus (A_1 \\cup A_2)\\\\ \u0026amp;\\vdots\\\\ B_t \u0026amp;= A_t \\setminus (A_1 \\cup \\cdots \\cup A_t)\\\\ \\end{align*}\nNotice that:\n$B_i \\cap B_j = \\varnothing, \\forall i \\neq j$ $B_i \\subseteq A_i, \\forall i$ $A_1 \\cup \\cdots \\cup A_t = B_1 \\cup \\cdots \\cup B_t$ These implies that: $$ \\mathbb{P}(A_1 \\cup \\cdots \\cup A_t) = \\sum_{i=1}^{t}\\mathbb{P}(B_i) \\leq \\sum_{i=1}^{t}\\mathbb{P}(A_i) $$\nThe binomial estimate $$ \\text{If } 1 \\leq k \\leq n, \\text{ then } {{n} \\choose {k}} \\leq \\left(\\frac{ne}{k}\\right)^k $$\nProof\nWe simply use that $1+x \\leq e^x$ for all $x \\in \\mathbb{R}$.\nTo see why this \u0026ldquo;fact\u0026rdquo; holds, consider the tangent line of the function $f(x) = e^x$ at $x=0$.\nUse the binomial theorem \\begin{align*} \\left( 1 + \\frac{k}{n} \\right)^k \u0026amp;= \\sum_{i=0}^{n}\\left(\\frac{k}{n}\\right)^i{{n} \\choose {i}} \u0026amp;\u0026amp;\\text{by Binomial Theorem}\\\\ \u0026amp;\\geq \\left( \\frac{k}{n} \\right)^k \\cdot \\binom{n}{k} \u0026amp;\u0026amp; \\text{when $i$ starts from $k$}\\\\ \\end{align*}\nSince we have the fact: $1+x \\leq e^x$, when $x = \\frac{k}{n}$, we have:\n\\begin{align*} \\left( e^{k/n} \\right)^n = e^k \\geq \\left( 1 + \\frac{k}{n} \\right)^k \u0026amp;\\geq \\left( \\frac{k}{n} \\right)^k \\cdot \\binom{n}{k} \\\\ \\iff \\left( \\frac{en}{k} \\right)^k \u0026amp;\\geq \\binom{n}{k} \\end{align*}\nNOW, lets start our proof of the \u0026ldquo;lower bound\u0026rdquo; theorem in the beginning Proof of $R(k)\\geq 2^{k/2}$\nIntuition We want to show that whenever $n$ is smaller than some number, it is possible to have some coloring of $K_n$ such that $\\nexists $monochromatic $K_k$.\nApproaching the proof Let $n \\in \\mathbb{N}$ to be chosen later.\nExperiment: color each edge of $K_n$ uniformly at random with red or blue.\nSet of all possible outcome is: $$ \\Omega = \\set{\\text{red, blue}}^{{n} \\choose {2}} $$\nThen, choose a $k$. Label all the copies of $K_k$ inside $K_n$. There are ${{n} \\choose {k}}$ of them.\nOur events will be of the following form: $$ A_i = \\set{i^{th} \\text{ clique is monochromatic}} $$\nConsider the probability of each of these $A_i$. They should have the same probability. So $\\forall i \\in \\set{1, \\ldots, \\binom{n}{k} }, A_i = A_1$. For each edge in the $K_k$ graph, it can be colored either red or blue. By applying the multiplication principle, this is: $$ \\mathbb{P}(A_i) = 2 \\cdot \\left( \\frac{1}{2} \\right)^{|E(K_k)|} = 2 \\cdot \\left( \\frac{1}{2} \\right)^{\\binom{k}{2}} $$\nWe want to know what is the probability of the event $A$, that for all coloring of $K_n$, there exists a monochromatic $K_k$:\n\\begin{align*} \\mathbb{P}\\left(A\\right) \u0026amp;= \\mathbb{P}\\left(\\bigcup_{i=1}^{\\binom{n}{k}}A_i\\right) = \\mathbb{P}\\left(\\bigcup_{i=1}^{\\binom{n}{k}}A_1\\right)\\\\ \u0026amp;= \\binom{n}{k}\\mathbb{P}\\left( A_1 \\right) = \\binom{n}{k} \\cdot 2 \\cdot \\left( \\frac{1}{2} \\right)^{\\binom{k}{2}}\\\\ \u0026amp;\\leq \\left( \\frac{ne}{k} \\right)^k \\cdot 2 \\cdot \\left( \\frac{1}{2} \\right)^{\\binom{k}{2}} \u0026amp;\u0026amp; \\text{binomial estimate}\\\\ \u0026amp;= \\left( \\frac{ne}{k} \\right)^k \\cdot 2 \\cdot \\left( \\frac{1}{2} \\right)^{k(k-1)/2}\\\\ \u0026amp;= \\left( \\frac{ne\\cdot 2^{1/k}}{k\\cdot 2^{(k-1)/2}} \\right)^k \\end{align*}\nHERE COMES THE MAGIC!!!\nPick $n = 2^{k/2}$ for this expression, then cancel some terms. \\begin{align*} P(A) \u0026amp;\\leq \\left( \\frac{2^{k/2} \\cdot e\\cdot 2^{1/k}}{k\\cdot 2^{(k-1)/2}} \\right)^k\\\\ \u0026amp;= \\left( \\frac{2^{1/2} \\cdot e \\cdot 2^{1/k}}{k} \\right)^k \\end{align*} Then, we argue that this is smaller than 1 for all $k \\geq 5$. To see why, just check the correctness of the inequality: $$ \\sqrt{2} \\cdot e \\cdot 2^{1/k} \u0026lt; k $$ When $k = 5$, left side is about 4.416. When $k$ gets larger, it left side gets closer to $\\sqrt{2} e$, which must be smaller than $k$. For $k \u0026lt; 5$ cases, we can just check them by hand.\nWhen we have the result that $P(A) \u0026lt; 1$, by the probabilistic method, we know the probability of the existance of a coloring such that no monochromatic $K_k$ appears in $K_n$.\nPersonally, I have one confusion: when $n = 2^{k/2}$, we have the above result. But we are trying to prove $R(k)\\geq 2^{k/2}$\u0026hellip; so\u0026hellip;🤔?\nReference: https://lmattos.web.illinois.edu/math-413-lecture-log/, MATH 413, Leticia Dias Mattos\n","permalink":"https://ohuro.me/posts/413_lower-bounded-ramsey/","summary":"Theorem $$ \\text{For every } k \\in \\mathbb{N}, \\text{we have } R(k)\\ge 2^{k/2} $$\nBefore we start the proof, let\u0026rsquo;s look at 3 ingredients we need:\nThe probabilistic method The union bound The binomial estimate The probabilistic method Let $\\Omega$ be the set of all possible outcomes of an experiment. Suppose that each outcome is equally likely. Let $A \\subseteq \\Omega$ be an event. $$ \\text{If } \\operatorname{Prob}(A) \u0026lt; 1 \\text{, then } \\operatorname{Prob}(A^c) \u0026gt; 0 $$","title":"413_lower bounded Ramsey"},{"content":"General Ramsey on normal graph Notation General notation for Ramsey theory:\n$K_n \\to (K_s,K_t)$ is the assertion that for any coloring of the edges of $K_n$ with red and blue, we can always find a red $K_s$ or a blue $K_t$.\n$K_n \\not \\to (K_s,K_t)$ is the assertion that there exists a coloring of the edges of $K_n$ with red and blue with no red $K_s$ nor blue $K_t$.\nNotation Off-diagonal Ramsey numbers:\nLet $s,t \\in \\set{1,2,3,\\ldots}$. The Ramsey number $R(s,t)$ is the smallest value of $n$ such that $K_n \\to (K_s,K_t)$.\n$R(s, t) = \\operatorname{min}\\set{n : \\forall c : E(K_n) \\to \\set{\\text{red, blue}}, \\exists \\text{ red } K_s \\text{ or blue }K_t}$\nObservation\n$R(s, t) = R(t, s)$ $R(2, t) = t$ Proof of observation 2:\n$R(2, t) = \\operatorname{min}\\set{n : \\forall c : E(K_n) \\to \\set{\\text{red, blue}}, \\exists \\text{ red } K_2 \\text{ or blue }K_t}$\nNeed to show two things:\n$\\exists c : E(K_{t-1}) \\to \\set{\\text{red, blue}} \\text{ s.t. } \\not \\exists \\text{ red } K_2 \\text{ or blue }K_t$. This holds since we have at most a blue $K_{t-1}$, the two coloring we want are both not satisfied. This property gives us $R(2, t) \\geq t$. $\\forall c : E(K_t) \\to \\text{red, blue, }\\exists \\text{ red } K_2 \\text{ or blue }K_t$. This holds since we either have one red edge, then done. Or all the edges are blue, then blue $K_t$, also done. This property gives us $R(2, t) \\leq t$. Theorem For all $s, t \\in \\mathbb{N}$, we have $R(s, t) \\leq R(s-1, t) + R(s, t-1)$.\n⚠️ Understanding this proof is important\u0026hellip; As we have a more generalized version of Ramsey\u0026rsquo;s theorem below, which has a similar proof. (In a same structure but using much more complicate notations.)\nProof\nSuppose you have a complete graph $K_n, n = R(s-1, t) + R(s, t-1)$. Now, fix the coloring $c : E(K_n) \\to \\set{\\text{red, blue}}$.\nGoal: to show that $\\exists $ red $K_s$ or blue $K_t$.\nTake a vertex $v$. Consider their red and blue neighbours of this $v$. Denote $N_R(v), N_B(v)$. We cannot have at the same time that:\n$|N_R(v) \\leq R(s-1,t) - 1|$ $|N_B(v) \\leq R(s,t-1) - 1|$ This is because, by contradiction, $n = N_R(v) + N_B(v) + 1 \\leq n-1$.\nTwo cases:\n$|N_R(v)| \\geq R(s-1, t)$ Inside $N_R(v)$, we can find $K_{s-1}$ red or $K_t$ blue. If $K_t$ blue, done. If $K_{s-1}$ red, also done since we have this $v$ here, adding an extra 1 to $K_{s-1}$.\n$|N_B(v)| \\geq R(s, t-1)$. This has a similar argument. Corollary\nFor all $s, t \\in \\mathbb{N}$, we have $R(s, t) \\leq {{s+t-2} \\choose {s-1}}$\nProof\nTwo observations:\n$R(2, t) = t$ $R(s, t) \\leq R(s-1, t) + R(s, t-1)$. By induction on $s+t$\nBase case: $s+t = 4$.\n$R(2, 2) \\leq {{2} \\choose {1}} = 2$ by (1)\n$R(1, 3) \\leq {{2} \\choose {0}} = 1$\nIH: Assume that it is true for $R(s-1, t)$ and $R(s, t-1)$. Let us show that this is true for $R(s, t)$.\n\\begin{align*} R(s, t) \\leq R(s-1, t) + R(s, t-1) \\\\ \\leq {{s+t-3} \\choose {s-2}} + {{s+t-3} \\choose {s-1}}\\\\ = {{s+t-2} \\choose {s-1}} \u0026amp;\u0026amp;\\text{by Pascal} \\end{align*}\nApplication: new proof for $R(t) \\leq 4^t$: $$ R(t, t) \\leq {{2t-2} \\choose {t-1}} \\leq 2^{2t-2} = \\frac{4^t}{4} $$\nRamsey for many colors on normal graph $K_n \\to (K_{t_1},K_{t_2},\\ldots,K_{t_k})$ is the assertion that for any coloring $c:E(K_n)\\to \\set{1,2,\\ldots,k}$ of the edges of $K_n$, there exists an $i$ so that we have a $i$-colored copy of $K_{t_i}$ under $c$.\nReminder: $E$ here is the notation for edges\nNotation Generalized Ramsey numbers:\nThe Ramsey number $R(t_1,\\ldots,t_k)$ is the smallest value of $n$ such that $K_n \\to (K_{t_1},\\ldots,K_{t_k})$.\nRamsey\u0026rsquo;s Theorem For any $k \\in \\mathbb{N}$ and any $t_1, t_2, \\ldots, t_k \\in \\mathbb{N}$, we have\n$$ R(t_1, \\ldots, t_k) \\leq \\infty $$\nConsider $R(t_1, t_2, t_3) \\leq R(R(t_1, t_2), t_3) \u0026lt; \\infty$\nWe want to prove the first inequality as the second one is trivial by definition.\nProof\nLet $n \\geq (R(R(t_1, t_2), t_3))$\nLet $c : E(K_n) \\to \\set{\\text{red, blue, green}}$ by any coloring. First, identify the red and blue color as black.\nGoal: to find red $K_{t_1}$ or blue $K_{t_2}$ or green $K_{t_3}$.\nAs $R(s, t) \u0026lt; \\infty \\implies R(R(t_1, t_2), t_3) \u0026lt; \\infty$\n$\\implies$ we find a black clique of size $R(t_1, t_2)$ in $K_n$ under $c$, or a green clique of size $K_{t_3}$.\nCase 1: We find the green $K_{t_3}$, we are done.\nCase 2: We find a black $K_{R(t_1, t_2)}$. In this graph, all edges are red or blue. We are garenteed to find $K_{t_1}$ red or $K_{t_2}$ blue.\nNOW, more general Ramsey on Hypergraph🙃 Instead of coloring edges(pair of vertices, they are sets of size two), we can color sets! (How cool is that 🙃🙃🙃)\nDefinition Hypergraph\nA hypergraph is a pair $(V, E)$ where $V$ is a set of vertices and $E$ is a collection of subset of $V$.\nThen the maximum number of hyperedges is just $|P(V) - 1|$. (empty set is not included)\nNotation $r$-uniform hypergraph\nLet $r \\in \\mathbb{N}$. We say that a hypergraph $\\mathcal{H}=(V,E)$ is $r$-uniform if all its hyperedges have $r$ vertices.\nDefinition Complete hypergraph\nLet $\\mathcal{H} = (V, E)$ be an $r$-uniform hypergraph on $n$ vertices. We say that $\\mathcal{H}$ is a complete hypergraph if every subset of $V$ of size $r$ is a hypergraph of $\\mathcal{H}$. That is: $$ E = \\set{A : A \\subseteq V \\text{ and } |A| = r} $$ Observe: $|E(\\mathcal{H})| = {{n} \\choose {r}}$\nNotation $K_n^r$ denotes the complete $r$-uniform hypergraph.\nNotation Ramsey for hypergraphs\n$K_n^r \\to (K_{t_1}^r,K_{t_2}^r,\\ldots,K_{t_k}^r)$ is the assertion that for any coloring $c:E(K_n)\\to \\set{1,2,\\ldots,k}$ of the hyperedges of $K_n^r$, there exists an $i$ so that we have a $i$-colored copy of $K^r_{t_i}$ under $c$.\nNotation Ramsey numbers for hypergraphs\nThe Ramsey number $R_r(t_1,\\ldots,t_k)$ is the smallest value of $n$ such that $K_n^r \\to (K_{t_1}^r,\\ldots,K_{t_k}^r)$.\nObservation\n$R_3(3, 10) = 10$\n$R_r(r, t) = t$\nAll of these are the analogies of Ramsey\u0026rsquo;s theories on normal graph with many colors.\nRamsey\u0026rsquo;s theorem for hypergraph For any $r, k \\in \\mathbb{N}$ and any $t_1, \u0026hellip;, t_k \\in \\mathbb{N}$ we have $$ R_r(t_1, \\ldots, t_k) \\leq \\infty $$ Proof\nInduction on $r$ and $t_1 + \u0026hellip; + t_k$\nQuestion: What should be the degree of a vertex so that in its neighbourhood we guarentee (1) $K_{t_1}^r$ 1-colored or (2)$K_{t_2}^r$ 2-colored or \u0026hellip;\nClaim: \\begin{align*} R_r(t_1,t_2, t_3) \u0026amp;\\leq R_{r-1}(R(t_1-1, t_2, t_3)) \\\\ \u0026amp;+ R_{r-1}(R(t_1, t_2-1, t_3)) \\\\ \u0026amp;+ R_{r-1}(R(t_1, t_2, t_3-1)) \\end{align*} Call the right side $n$.\nFix any coloring $c : E(K_n^r) \\to \\set{\\text{red, blue, green}}$\nGoal: to find red $K_{t_1}^r$ or blue $K_{t_2}^r$ or green $K_{t_3}^r$\nBy pigeonhole principle:\nLet\u0026rsquo;s define the coloring:\n$c\u0026rsquo; : E(K_{n-1}^{r-1}) \\to \\set{\\text{red, blue, green}}$ as follows:\n$c\u0026rsquo;(e) = $ $c(v \\cup e)$ ==The detailed proof goes in the homework\u0026hellip;==\nNow, there is one interesting application of this Ramsey\u0026rsquo;s theorem for hypergraph.\nThe happy ending problem Let $A$ be a set of $n$ points in $\\mathbb{R}^2$ such that no three points of $A$ are co-linear. If $n$ is sufficiently large, then $A$ contains $k$ points forming a convex $k$-gon.\nSolved by Erdos-Szekeres.\nDefinition The definition of a convex polygon is a bit complicated. But they have this property:\nIn a convex polygon, all interior angles are less than or equal to 180 degrees, while in a strictly convex polygon all interior angles are strictly less than 180 degrees. (https://en.wikipedia.org/wiki/Convex_polygon)\nSome simple cases:\nQuestion: How many points in general position guarantee a convex 3-gon? THREE!\nHow many points in general position guarantee a convex 4-gon? Four is not enough. Consider an arrow like 4-gon.\nSuppose we have 5 points in general position (g.p. means no 3 are colinear). Whenever we have a convex 4-gon or 5-gon, we are done. Otherwise, in case (1), draw a line inside the triangle, it crosses two edges. We can see that the two any two points on the line, together with other two points, will form a 4-gon. (I don\u0026rsquo;t know how to argue this using a formal geometry proof, but the intuition should be similar to this.)\nGeneral Proof\nConsider any set of $n = R_4(k,k)$ points in general position: $p_1, \\ldots, p_n$.\nWe define a coloring of $K_n^4$ as follows:\nColor $p_1, p_2, p_3, p_4$ blue if these points are in convex position. Color them red if they are not covex. Then Ramsey\u0026rsquo;s Theorem tells us that: $\\exists$ monochromatic $K_k^4$.\nQuestion: Can all hyperedges be red? What happenwith 5 points in general position?\n$K \\geq 5 \\implies \\exists$ convex 4-gon $\\implies$ blue edges $\\implies$ the monochromatic $k$-clique is blue!\nConclusion: we can find $k$ points $p_1, \\ldots, p_k$ so that every 4-points are in convex position.\nBUT, what\u0026rsquo;s next? Hint: loook at the convex hull.\nCase 1: If we find all these $k$ points forms a convex $k$-gon (case 1), then we are happy.\nOtherwise, in case 2: Choose one vertex, and draw lines to other verticies. If there is a point \u0026ldquo;inside\u0026rdquo;, it must be inside one of the triangles created. Then those four points forms a non-convex 4-gon, which contradicts our conclution. Then only case 1 exists. $\\blacksquare$\nWish you a good dream without Ramsey😴 Reference: https://lmattos.web.illinois.edu/math-413-lecture-log/, MATH 413, Leticia Dias Mattos\n","permalink":"https://ohuro.me/posts/413_general_ramsey-theories/","summary":"General Ramsey on normal graph Notation General notation for Ramsey theory:\n$K_n \\to (K_s,K_t)$ is the assertion that for any coloring of the edges of $K_n$ with red and blue, we can always find a red $K_s$ or a blue $K_t$.\n$K_n \\not \\to (K_s,K_t)$ is the assertion that there exists a coloring of the edges of $K_n$ with red and blue with no red $K_s$ nor blue $K_t$.\nNotation Off-diagonal Ramsey numbers:","title":"413_General Ramsey Theories"},{"content":"Erdos-Szekeres Theorem Given $r, s$ any sequence of distinct real numbers with length at least $(r-1)(s-1)+1$ contains a monotonically increasing subsequence of length $r$ or a monotonically decreasing subsequece of length $s$.\nThat is, given the subsequence: $a_1, a_2, \u0026hellip;, a_{(s-1)(r-1)+1}$, we can find:\nIndicies $i_1 \u0026lt; i_2 \u0026lt; \u0026hellip;\u0026lt; i_r$ so that $a_{i1} \u0026lt; a_{i2} \u0026lt; \u0026hellip;\u0026lt; a_{ir}$ OR\nIndicies $j_1 \u0026lt; j_2 \u0026lt; \u0026hellip;\u0026lt; j_r$ so that $a_{j1} \u0026gt; a_{j2} \u0026gt; \u0026hellip; \u0026gt; a_{jr}$ Example: $s = 3, r = 3$, Sequence: $10, 11, 7, 8$ has no such property.\nE-S thm is best possible:\n$\\exists$ subsequence of length $(s-1)(r-1)$ so that $\\not \\exists$ increasing subsequence of length $r$ nor a decreasing subsequence of length $s$.\nProof\nThe subsequence: $a_1, a_2, \u0026hellip;, a_{(s-1)(r-1)+1}$.\nFor each $i \\in \\set{1, \u0026hellip;, (s-1)(r-1)+1}$, define a pair $(x_i, y_i)$ as follows:\n$x_i = $ the length of the longest increasing subsequence $a_1, \u0026hellip;, a_i$\n$y_i = $ the length of the longest decreasing subsequence $a_1, \u0026hellip;, a_i$\nExample: subsequence $1, 4, 5, 2, 7, 3$\n\\begin{align*} (x_1, y_1) = (1, 1)\\\\ (x_2, y_2) = (2, 1)\\\\ (x_3, y_3) = (3, 1)\\\\ (x_4, y_4) = (3, 2)\\\\ \\end{align*}\nClaim 1: Let $i \\in \\set{1, \u0026hellip;, (s-1)(r-1)}$\nThen, either $x_{i+1} \u0026gt; x_i$ or $y_{i+1} \u0026gt; y_i$\nProof of claim 1:\nLet $a_{j1}, a_{j2}, \u0026hellip;, a_{jx_i}$ be the longest increasing subsequence in $a_1, \u0026hellip;, a_i$, ending with $a_i$. So $a_{jx_i} = a_i$\nLet $a_{t1}, a_{t2}, \u0026hellip;, a_{ty_i}$ be the longest decreasing subsequence in $a_1, \u0026hellip;, a_i$, ending with $a_i$. So $a_{ty_i} = a_i$\nTwo options:\n$a_{i+1} \u0026gt; a_i$, then it contributes to the increasing subsequence: $a_{j1}, \u0026hellip;, a_{jx_i}, a_{i+1}$ $a_{i+1} \u0026lt; a_i: a_{t_1} , \u0026hellip;, a_{ty_i}, a_{i+1}$ Note that here, $a_{ty_i}$ and $a_{jx_i}$ are both equal to $a_i$\nClaim 2: Let $i, j \\in \\set{1, \u0026hellip;, (s-1)(r-1) + 1}$ with $i \u0026lt; j$. Then either $x_j \u0026gt; x_i$ or $y_j \u0026gt; x_i$. This gives us: $(x_i, y_i) \\neq (x_j, y_j), \\forall j \\neq i$\nProof of claim 2:\nSame notation as claim 1:\nLet $a_{j1}, a_{j2}, \u0026hellip;, a_{jx_i}$ be the longest increasing subsequence in $a_1, \u0026hellip;, a_i$, ending with $a_i$. So $a_{jx_i} = a_i$\nLet $a_{t1}, a_{t2}, \u0026hellip;, a_{ty_i}$ be the longest decreasing subsequence in $a_1, \u0026hellip;, a_i$, ending with $a_i$. So $a_{ty_i} = a_i$\nThere are two options:\n$a_j \u0026gt; a_i$. Then we can take $a_j, \u0026hellip;, a_{jx_i}, a_j$ , which give us longer increasing $x_j \u0026gt; x_i$. $a_j \u0026lt; a_i$. Then we can take $a_t, \u0026hellip;, a_{ty_i}, a_j$ , which give us longer decreasing $y_j \u0026gt; y_i$. Now that the two claims are done, we can construct this box that contains $(r-1)(s-1)$ points. Then the extra one point in the 2D space gives us the fact that along one dimension, we have $s$ or $r$ points.\n$\\blacksquare$\nGraph \u0026amp; Ramsey\u0026rsquo;s theories Definition\nA graph is an ordered pair $G = (V, E)$ comprising:\n$V$, a set of vertices (also called nodes or points)\n$E \\subseteq \\set{{x,y}: x,y \\in V \\text{ and } x \\neq y}$, a set of edges (also called links or lines), which are unordered pairs of vertices.\nDefinition\nA complete graph is a graph in which each pair of graph vertices is connected by an edge.\nThe complete graph with $n$ vertices is denoted by $K_n$.\nExercise\nWhat is the number edges in $K_n$?\n$\\sum_{i=1}^{n-1} i$, or simply ${{n} \\choose {2}}$\nTheorem of Ramsey Of six (or more) people, either there are three, each of pair of whom are acquainted, or there are three, each pair of whom are unaquainted.\nIn graph theory language: for any coloring of the edges of $K_6$ with red or blue, then there exists at least one monochromatic triangle $K_3$.\nQuestion: How many ways to color $K_6$?\nThere are $e = {{6} \\choose {2}} = 15$ edges, so the number of ways is $2^{15}$. It would be horrible to brute force checking all the possibilities.\nProof\nLabeling the graph verticies from 1 to 6.\nThere are 5 edges starting from 1. By pigeonhole principle, $\\exists$ at least 3 edges of the same color. Then there are two cases:\n$\\exists$ at least one red edge among the three verticies connected to 1, forming a red triangle. Otherwise, they are all blue, forming a blue triangle. Notation: $K_n \\to K_s$ is the assertion that for any coloring of the edges of $K_n$ with red and blue, we can always find a monochromatic $K_s$.\n$K_n \\not \\to K_s$ is the assertion that there exists a coloring of the edges of $K_n$ with red and blue with no monochromatic $K_s$.\nNotation Ramsey number\nLet $t \\in \\set{1, 2, 3,\u0026hellip;}$. The Ramsey number $R(t)$ is the smallest value of $n$ such that $K_n \\to K_t$.\nRamsey\u0026rsquo;s theorem For every $k \\in \\mathbb{N}$, we have $R(k) \\leq 4^k$.\nProof: $k$ is fixed\nTake $n = 4^k$. Take one vertex, label it $v_1$. $v_1$ has $4^k - 1$ neighbors.\nNote that either $|N_R(v_1)| \\geq \\frac{4^k}{2}$ or $|N_B(v_1)| \\geq \\frac{4^k}{2}$. (By pigeonhole)\n$N_R(v_1) = \\set{u:v_1u \\text{ is red}}$\n$N_B(v_1) = \\set{u:v_1u \\text{ is blue}}$\nStep 1: Let us color $v_1$ blue if $|N_B(v_1)| \\geq \\frac{4^k}{2}$ and red otherwise.\nThen, restrict our graph to $v_1 \\cup N_{c(v_1)}(v_1)$.\nStep 2: Take $v_2 \\in N_{c(v_1)}(v_1)$. Color $v_2$ blue if $|N_B(v_2) \\cap N_{c(v_1)}(v_1)| \\geq \\frac{4^k}{4}$, otherwise, color it red.\nBecause, by pigeonhole, either:\n$|N_B(v_2) \\cap N_{c(v_1)}(v_1)|\\geq \\frac{4^k}{2} / 2$ $|N_R(v_2) \\cap N_{c(v_1)}(v_1)|\\geq \\frac{4^k}{2} / 2$ $\\vdots$\nStep $i+1$:\nTake $v_{i+1} \\in A_i = N_{c(v_1)}(v_1) \\cap N_{c(v_2)}(v_2) \\cap \\cdots \\cap N_{c(v_i)}(v_{i+1})$\nNote that $|A_i| \\geq \\frac{4^k}{2^i}$. By pigeonhole, either:\n$|N_B(v_{i+1})| \\geq \\frac{4^k}{2^{i+1}}$ $|N_R(v_{i+1})| \\geq \\frac{4^k}{2^{i+1}}$ Now when we get to $i = 2k-1, |A_{i-1}| \\geq \\frac{4^k}{2^{2k-1}} = 2$. So we can still color the vertex $v_{i+1}$\nAccording to vertex coloring of $v_1, v_2, \\cdots, v_{i}$, select majority color for $v_{i+1}$. This gives us a monochromatic $K_k$ by pigeonhole principle.\nReference: https://lmattos.web.illinois.edu/math-413-lecture-log/, MATH 413, Leticia Dias Mattos\n","permalink":"https://ohuro.me/posts/413_erdos-szekeres-theorem-and-ramsey-theory/","summary":"Erdos-Szekeres Theorem Given $r, s$ any sequence of distinct real numbers with length at least $(r-1)(s-1)+1$ contains a monotonically increasing subsequence of length $r$ or a monotonically decreasing subsequece of length $s$.\nThat is, given the subsequence: $a_1, a_2, \u0026hellip;, a_{(s-1)(r-1)+1}$, we can find:\nIndicies $i_1 \u0026lt; i_2 \u0026lt; \u0026hellip;\u0026lt; i_r$ so that $a_{i1} \u0026lt; a_{i2} \u0026lt; \u0026hellip;\u0026lt; a_{ir}$ OR\nIndicies $j_1 \u0026lt; j_2 \u0026lt; \u0026hellip;\u0026lt; j_r$ so that $a_{j1} \u0026gt; a_{j2} \u0026gt; \u0026hellip; \u0026gt; a_{jr}$ Example: $s = 3, r = 3$, Sequence: $10, 11, 7, 8$ has no such property.","title":"413_Erdos-Szekeres Theorem and Ramsey Theory"},{"content":"Permutation of Sets Definition permutation A permutation of a set $S$ can be thought of as a listing of the elements $S$ in some order.\n$P(n, r)$ denotes the number of $r$-permutations of an $n$-element set.\nDefinition $r$-permutation\nAn $r$-permutation of a set $S$ is a string $a_1, a_2, \u0026hellip; a_r$ where $a_i \\in S, \\forall i \\in [r]$ and $a_i \\neq a_j$ for $i \\neq j$.\nNote: $[r] = {1, 2, \u0026hellip;, r}$\nTheorem For $n$ and $r$ positive integers with $r \\leq n$, $$ P(n, r) = n \\cdot (n - 1) \\cdot \\cdot \\cdot(n - r + 1) $$ Proof by multiplication principle.\nWIth the notation of factorial, we can write: $$ P(n, r) = \\frac{n!}{(n - r)!} $$\nExercise How many ways are there to label the squares for the $4 \\times 4$ chessboard with the numbers $1,2, \u0026hellip;, 15$, in such a way that exactly one square does not receive any label?\nsolution: Consider labeling the empty square with 16\u0026hellip;\nExercise What is the number of ways to order the 26 letters so that no two vowels a, e, i, o, u occurs consecutively?\nLeft as an exercise.\nExercise How many seven-digit numbers are there such that the digits are distinct integers taken from $\\set{1, 2, \u0026hellip;, 9}$ and such that the digits 5 and 6 do not appear consecutively in either order?\nsolution: $m$ = The number of all possible combination = $\\frac{9!}{2!}$.\n$k$ = The number of strings of size 6 with distinct elements in $\\set{1, 2, 3, 4, 7, 8, 9, x}$ and containning $x$, where $x = 56$. $= 6 \\cdot 7 \\cdot 6 \\cdot 5 \\cdot 4 \\cdot 3$\nThe first 6 is for the position to place $x$.\nAns = $m - 2k$\nCircular permutation Theorem The number of circular $r$-permutation of a set of $n$ elements is given by:\n$$ \\frac{P(n, r)}{r} = \\frac{n!}{r \\cdot (n-r)!} $$ Proof Similar to the line permutation. We divide the number by $r$ because the same circle is counted $r$ times for all circles.\nReference: https://lmattos.web.illinois.edu/math-413-lecture-log/, MATH 413, Leticia Dias Mattos\nIF YOU KNOW LESS FORMULAS, YOU WOULD HAVE MORE SPACES IN YOUR BRAIN. 😝 ","permalink":"https://ohuro.me/posts/413_permutation/","summary":"Permutation of Sets Definition permutation A permutation of a set $S$ can be thought of as a listing of the elements $S$ in some order.\n$P(n, r)$ denotes the number of $r$-permutations of an $n$-element set.\nDefinition $r$-permutation\nAn $r$-permutation of a set $S$ is a string $a_1, a_2, \u0026hellip; a_r$ where $a_i \\in S, \\forall i \\in [r]$ and $a_i \\neq a_j$ for $i \\neq j$.\nNote: $[r] = {1, 2, \u0026hellip;, r}$","title":"413_permutation"},{"content":"Texture Synthesis \u0026amp; Hole Filling Texture Texture depicts spacially repeating patterns.\nTexture Synthesis Create new samples of a given texture. Many applications: virtual environments, hole-filling, texturing surfaces.\nThe challenge:\nNeed to model the whole spectrum: from repeated to stochastic texture.\nOne idea:\nCompute statistics of input texture Generate a new texture that keeps those same statistics. But it is hard to model those probabilities distributions.\nAnother idea: ==Efros \u0026amp; Leung algorithm==\nHow to match patches?\nGaussian-weighted SSD (sum square difference) gives us more emphasis on nearby pixels. $$ \\text{SSD}(P, Q) = \\sum_{i, j}(p_{ij} - q_{ij})^2 w_{ij}\\ \\text{where } w_{ij} = e^{ \\frac{-(1-w/2)^2 - (j - h/2)^2}{2 \\sigma^2} } $$\nwhere $P$, and $Q$, are two patches, $w$ and $h$ in the $w_{ij}$ is the width and height of the patch.\nWhat order to fill in new pixels?\n\u0026ldquo;Onion skin\u0026rdquo; order: pixels with most neighbors are synthesized first. How big should the patches be? The size of neighborhood window decides how stochasticity of the texture. A smaller window size gives a more random output.\n$\\boldsymbol{\\textsf{Texture synthesis algorithm}}$ While image not filled:\nGet unfilled pixels with filled neighbors, sorted by the numebr of filled neighbors. (priority queue?) For each pixel, get top N matches based on visible neighbors. This is where we use the Gaussian-weighted SSD. Randomly select one of the matches and copy pixels from it. This algorithm can be used for hole filling, extrapolation, \u0026hellip;\nHole Filling Sometimes, we can add more weights for the continuos edges when peforming the onion filling. (Gradient sensitive)\nThe Efros \u0026amp; Leung texture synthesis algorithm is simple and good, but too slow\u0026hellip;\nThe next iteration is: Image quilting (Efros \u0026amp; Freeman 2001).\nIt depends on the observation that: neighbor pixels are highly correlated. Now, instead of filling pixel by pixel, we fill block by block.\nWe need to put the tiles together. To make them look seemless, we can use this minimal error boundary cut. We calculate the square difference of the overlapping part, and calcualte the boundary. Using this simplified Dijikstra\u0026rsquo;s algorithm, we can easy calculate what we want.\n","permalink":"https://ohuro.me/posts/445_texture/","summary":"Texture Synthesis \u0026amp; Hole Filling Texture Texture depicts spacially repeating patterns.\nTexture Synthesis Create new samples of a given texture. Many applications: virtual environments, hole-filling, texturing surfaces.\nThe challenge:\nNeed to model the whole spectrum: from repeated to stochastic texture.\nOne idea:\nCompute statistics of input texture Generate a new texture that keeps those same statistics. But it is hard to model those probabilities distributions.\nAnother idea: ==Efros \u0026amp; Leung algorithm==","title":"445_texture"},{"content":"Pigeonhole Principle If $n+1$ objects are distributed into $n$ boxes, then at least one box contains two or more of the objects.\nProof\nFor each $i\\in \\set{1, \u0026hellip;, n}$, let $a_i = $ number of objects in box $i$. Then, $a_1 + \u0026hellip; + a_n = n+1$ Let $a_j = \\text{max } a_i$. Then, \\begin{align*} n+1 \u0026amp;\\leq n\\cdot a_j \\\\ \\frac{n+1}{n} \u0026amp;\\leq a_j \\\\ 1 \u0026lt; \\frac{n+1}{n} \u0026amp;\\leq a_j\\\\ 1 \u0026amp;\u0026lt; a_j \\end{align*} Note that this conclusion doesn\u0026rsquo;t hole if you have $n$ objects or less.\nExercise There are 5 points in a square of side length 2. Prove that at least two of them are with the distance at most $\\sqrt{2}$.\nEqually divide this square into four smaller squares with side length 1. By the pigeonhole principle, $\\exists$ one $1 \\times 1$ square with $ \\geq$ 2 points.\nTheorem A grid of 27 points (forming a $3 \\times 9$ square) in the plane is given.Each point is coloured red or blue. Prove taht there exists a monochromatic rectangle, that is, a rectangle with all four vertices of the same color.\nProof\nConsider the the pattern of coloring of a single column. Each colum has 3 points. Then, there are $2^3 = 8$ ways of coloring this column. As there are 9 columns in this square, by pigeonhole principle, there are at least two columns in the same pattern. Again, by the pigeonhole principle, each column must have at least two red or two blue.\nMore abstract formulations of the pigeonhole principle Let $X$ and $Y$ be finite sets and let $f: X \\to Y$ be a function from $X$ to $Y$.\nProposition If $X$ has more elements than $Y$, then $f$ is not injective.\nProof\nLet $X = \\set{x_1, \u0026hellip;, x_n}$, $Y = \\set{y_1, \u0026hellip;, y_m}$\nDefine $A_i = \\set{x_j : f(x_j) = y_i}, \\forall i \\in \\set{1, \u0026hellip;, m}$.\nThen we have: $n = |X| = |A_1| + \u0026hellip; + |A_m|$\nSuppose for contradiction that $f$ is injective\n$\\implies |A_i| \\leq 1 \\implies |X| = |A_1| + \u0026hellip; + |A_m| \\leq m \\implies n \\leq m$. Contradiction\nProposition If $X$ and $Y$ have the same number of elements and $f$ is surjective, then $f$ is injective.\nProof $n = |X| = |A_1| + \u0026hellip; + |A_m| \\geq m$ by the definition of surjective. And since $m=n$, $|A_1| + \u0026hellip; + |A_m| = m \\implies |A_i| = 1$\nProposition If $X$ and $Y$ have the same number of elements and $f$ is injective, then $f$ is surjective.\nTheorem Let $m$ be any positive integer and let $a_1, \u0026hellip;, a_m$ be a sequence be any sequance of $m$ integers. Then, there exsist consecutive $a$\u0026rsquo;s in the sequence whose sum is divisible by $m$.\nThat is, there exists integers $k$ and $l$ with $0 \\leq k \\leq m$ for which $a_{k+1} + a_{k+1} + \u0026hellip; + a_l$ is divisible by $m$.\nProof\nPigeons: \\begin{matrix} s_1 =\u0026amp; a_1\\\\ s_2 =\u0026amp; a_1 + a_2\\\\ \\vdots\\\\ s_m =\u0026amp; a_1 + a_2 + ... + a_m \\end{matrix}\nPigeonholes: $$ A_r = \\set{n \\in \\mathbb{Z} : n = qm + r, q \\in \\mathbb{Z}}, \\text{where } r \\in \\set{1, \u0026hellip;, m} $$\nThe rule is to put $s_i$ into $A_r$ if $s_i \\equiv r \\mod{m}$.\nNotice that we select $r$ from $1$ to $m$. This is because:\nWhenever we have some $k \\in \\set{1, .., m}$ such that $a_1 + \u0026hellip; + a_k = qm = qm + 0$, we have done finding our goal sequence.\nOtherwise, putting $m$ pigeons into $m-1$ pigeonholes gives us a fact that $\\exists i, j \\in \\set{1, \u0026hellip;, m}, i \\neq j$ such that $(a_1 + \u0026hellip; + a_i) \\equiv (a_1 + \u0026hellip; a_j) \\text{ mod }m $. WLOG, suppose $i \u0026lt; j$, we have $m \\mid (a_{i+1} + \u0026hellip; + a_j)$, giving us what we want.\nExercise\nA chess master who has 11 weeks to prepare for a turnament decies to play at least one game everday. But to avoid tiring himself, he decides not to play more than 12 games during any calendar week. Show that there exists a succession of days during which the chess master will have played exactly 21 games.\nSolution:\n$77 \\leq \\text{number of games} \\leq 132$\nDefine the sequence: \\begin{align*} a_1 \u0026amp;= \\text{\\# games played at day 1}\\\\ a_2 \u0026amp;= \\text{\\# games played at day 1 and day 2}\\\\ \\vdots\\\\ a_{77} \u0026amp;= \\text{\\# games played from day 1 to day 77} \\end{align*} Define another sequence:\n\\begin{align*} \u0026amp;a_1 + 21\\\\ \u0026amp;a_2 + 21\\\\ \u0026amp;\\vdots\\\\ \u0026amp;a_{77}+21 \\end{align*}\nPut these two sequences together, we have a sequance of size $77 \\cdot 2 = 154$\nThe goal is to show that two of them are equal. Note that there are 132 games at maximum, then the maximum number in this $132 + 21 = 153$.\nBy the pigeonhole pirinciple, two of them are equal. And since these two equal number may not come from the same sequence, $\\exists a_i = a_j + 21$, which gives us the consecutive days $i$ to $j$.\nChinese Remainder Theorem Let $m$ and $n$ be relatively prime positive integers, and let $a$ and $b$ be integers where $0 \\leq a \\leq m - 1$ and $0 \\leq b \\leq n - 1$.\nThen, there is a positive integer $x$ such that $x = pm + a = qn + b$ for some integer $p,q$.\nProof\nConsider the sequence that has $n$ numbers in this sequence. $$ a, m + a, 2m+a, \u0026hellip;, (n-1)m + a $$\nClaim: No two numbers in this sequence have same remainder division by $n$.\nSuppose, by contradiction, that $\\exists i\u0026lt; j, p, q, \\in \\mathbb{Z}, \\text{and } r \\in \\set{0, \u0026hellip;, n-1}$ such that:\n$im + a = pn + r$ and $jm + a = qn + r$. This gives us: \\begin{align*} (j - i)m = n(q-p)\\\\ \\implies n \\mid (j - i)m\\\\ \\implies n \\mid (j-i) \\end{align*} As $i, j \\leq n - 1 \\implies 0 \\leq j-i \\leq n-1 \\implies j-i = 0$, giving us a contradiction\nThen the claim is true.\nTherefore, all the remainders show up $\\implies$ there exists $i$ such that $im + a = pn + b, p \\in \\mathbb{Z}$. $\\blacksquare$\nStrong Form of Pigeonhole Principle Let $q_1, q_2, \u0026hellip;, q_n$ be positive integers. If $q_1 + \u0026hellip; + q_n - n + 1$ objects are distributed into $n$ boxes, then either the first box contains at least $q_1$ objects, or the second box contains at least $q_2$ objects,\u0026hellip; or the $n$ th box contains at least $q_n$ objects.\nProof\nSuppose for contradiction that:\nBox 1 recieves $\\leq q_1 - 1$ Box 2 recieves $\\leq q_2 - 1$ \u0026hellip; Box $n$ recieves $\\leq q_n - 1$ Which sums to $(q_1 + .. + q_n) - n$\nAs we have the number of objects = $q_1 + \u0026hellip; + q_n - n + 1$\nThen the contradiction happens. $\\blacksquare$\nCorollary\nIf $n(r-1)+1$ objects are distributed into $n$ boxes, then at least one of the boxes contains $r$ or more of the objects.\nProof\nTake $q_1 = q_2 = \u0026hellip; = q_n = r$\n$n(r - 1) + 1 = nr - n + 1 = (q_1 + \u0026hellip; + q_n) - n + 1$\nBy theorem, there must be one box $\\geq r$. $\\blacksquare$\nAnother way to formulate the pigeonhole principle Let $m_1, m_2, \u0026hellip;, m_n$ be non-negative integers. if $$ \\frac{m_1 + \u0026hellip; + m_n}{n} \u0026gt; r - 1 $$ then at least one of the integers is greater or equal to $r$.\nProof\nBy contradiction, suppose that $m_i \u0026lt; r, \\forall i$. We can take the maximum value: $mi = r-1, \\forall i$, which gives us: \\begin{align*} \\sum_{i=1}^{n} m_i = n(r-1)\\\\ \\frac{\\sum_{i=1}^{n} m_i}{n} = (r-1) \\end{align*} Clearly, there is a contradiction. $\\blacksquare$\nExercise\nTwo disks, one smaller than the other, are each divided into 200 congruent sectors.\nIn the larger disk, 100 of the sectors are chosen arbitrarily and painted red; the other 100 sectors are painted blue.\nIn the smaller disk, each sector is painted either red or blue with no stipulation on the number of red and blue sectors.\nThe small disk is then placed on the larger disk so that their centers coincide.\nShow that it is possible to align the two disks so that the number of sectors of the small disk whose color matches the corresponding sector of the large disk is at least 100.\nText copied from professor Leticia Dias Mattos\u0026rsquo;s Slide: https://lmattos.web.illinois.edu/math-413-lecture-log/\nProof\nDefine: a configuration $c$ is just a way of placing the small disk on the large disk with aligned colors.\nWe can label (or mark the position of) the disk from 1 to 200\nDouble counting the following set: $$ S = \\set{(c, i): c \\text{ is a configuration and the sector } i \\text{ have matching colors}} $$ (1)\nFor each $i$, let $t_i = $ number of configurations so that the sector $i$ has matching colors\nThen $|S| = \\sum_{i=1}^{200} t_i = 200 \\cdot 100$\n(2)\nFor each configuration $c$, let $a_c$ be the number of sectors that has same color in the same label(position)\n\\begin{align*} |S| \u0026amp;= \\sum_{\\text{c config}}^{} a_c\\\\ \\frac{|S|}{200} \u0026amp;= \\frac{\\sum_{\\text{c config}}^{} a_c}{200} = 100 \u0026gt; 100-1 \\end{align*} By pigeonhole principle, done. $\\blacksquare$\nReference: https://lmattos.web.illinois.edu/math-413-lecture-log/, MATH 413, Leticia Dias Mattos\n","permalink":"https://ohuro.me/posts/413_pigeonhole/","summary":"Pigeonhole Principle If $n+1$ objects are distributed into $n$ boxes, then at least one box contains two or more of the objects.\nProof\nFor each $i\\in \\set{1, \u0026hellip;, n}$, let $a_i = $ number of objects in box $i$. Then, $a_1 + \u0026hellip; + a_n = n+1$ Let $a_j = \\text{max } a_i$. Then, \\begin{align*} n+1 \u0026amp;\\leq n\\cdot a_j \\\\ \\frac{n+1}{n} \u0026amp;\\leq a_j \\\\ 1 \u0026lt; \\frac{n+1}{n} \u0026amp;\\leq a_j\\\\ 1 \u0026amp;\u0026lt; a_j \\end{align*} Note that this conclusion doesn\u0026rsquo;t hole if you have $n$ objects or less.","title":"413_Pigeonhole Principle"},{"content":"Binomial Coefficient and Binomial Identity Pascal\u0026rsquo;s Triangle \\begin{matrix} {0 \\choose 0}\\\\ {1 \\choose 0} \u0026amp; {1 \\choose 1}\\\\ {2 \\choose 0} \u0026amp; {2 \\choose 1} \u0026amp; {2 \\choose 2}\\\\ {3 \\choose 0} \u0026amp; {3 \\choose 1} \u0026amp; {3 \\choose 2} \u0026amp; {3 \\choose 3}\\\\ {4 \\choose 0} \u0026amp; {4 \\choose 1} \u0026amp; {4 \\choose 2} \u0026amp; {4 \\choose 3} \u0026amp; {4 \\choose 4}\\\\ .\\\\ .\\\\ . \\end{matrix}\nPascal\u0026rsquo;s Formula For all integers $n$ and $k$ with $1 \\leq k \\leq n - 1$, we have: $$ {n \\choose k} = {{n-1} \\choose k} + {{n-1} \\choose {k-1}} $$\nProof\nLet $S = \\set{1, 2, \u0026hellip;, n}$ be a set of size n.\nWe\u0026rsquo;ll count in two ways of the size of the following set:\n$\\mathcal{C} = \\set{A : A \\subseteq S, |A| = k}$\n(1) $|\\mathcal{C}| = {n \\choose k}$\n(2) Define: \\begin{align*} \\mathcal{C_1} = \\set{A : A \\subseteq S, |A| = k, 1 \\in A}\\\\ \\mathcal{C_2} = \\set{A : A \\subseteq S, |A| = k, 1 \\not\\in A} \\end{align*} Note that: $\\mathcal{C_1}$ and $\\mathcal{C_2}$ are disjoint, $\\mathcal{C} = \\mathcal{C_1} \\cup \\mathcal{C_2}$\nSince $|\\mathcal{C_1}| = {{n-1} \\choose {k-1}}$, $|\\mathcal{C_2}| = {{n-1} \\choose {k}}$, we get the identity.\nTheorem For $n \\geq 0$, we have: $$ {n \\choose 0} + {n \\choose 1} + {n \\choose 2} + \u0026hellip;+{n \\choose n} = 2^n $$ This is a row in the Pascal\u0026rsquo;s triangle.\nProof\nLet $\\mathcal{C}$ = $n$-digit binary numbers. Two way to count this:\n(1) $|\\mathcal{C}| = 2^n$\n(2) Define: $\\mathcal{C_i} = n$-digit binary numbers with exactly $i$ 1\u0026rsquo;s.\nThen simply $|\\mathcal{C}| = \\sum_{i=0}^{n}\\mathcal{C_i}$ , as they are all disjoint.\nTheorem $$ {n \\choose 0} + {n \\choose 2} + \u0026hellip; = {n \\choose 1} + {n \\choose 3} + \u0026hellip; $$\nProof\nLet $S = \\set{1, \u0026hellip;,n}$\nDefine:\n$\\mathcal{C_1} = $ subsets of $S$ with odd size\n$\\mathcal{C_2} = $ subsets of $S$ with even size\nQuestion: How to select an even subset?\nFirst, select any subset $A \\subseteq \\set{1, \u0026hellip;, n - 1}$:\nIf $|A|$ is even, then $A \\in \\mathcal{C_2}$, and $A \\cup \\set{n} \\in \\mathcal{C_1}$ If $|A|$ is odd, then $A \\in \\mathcal{C_1}$, and $A \\cup \\set{n} \\in \\mathcal{C_2}$ As this constructs a bijection, $|\\mathcal{C_1}| = |\\mathcal{C_2}| = 2^{n-1}$\nThe Binomial Theorem Let $n$ be a positive integer. Then, for all $x$ and $y$ we have: $$ (x+y)^n = \\sum_{k=0}^{n} {n \\choose k} x^{n-k}y^k $$ Proof\nAs we have $n$ $(x+y)$ multiplication, we choose either an $x$ or a $y$ from each pair to multiply. This makes sure that we must have all terms in the form: $x^iy^{n-i}$. The remaining task is simply count the number of $x^iy^{n-i}$. Given this $i$, we have $n \\choose i$ ways to choose the combination, thus we have the resulting formula.\nCorollary\nLet $n$ be a positive integer. Then, for all $x$ we have\n$$ (1+x)^n = \\sum \\limits_{k=0}^{n} \\binom{n}{k} x^k $$\nTheorem For $1 \\leq k \\leq n$, we have: $$ {n \\choose k} = \\frac{n}{k} {{n - 1} \\choose {k-1}} $$ Proof\nGoal: $k{n \\choose k} = n{{n - 1} \\choose {k-1}}$\nAmong $n$ people, we want to select $k$ people and we want to elect one of these $k$ people as the boss.\n(1) First select $k$ people, then select the boss: ${n \\choose k} \\cdot k$\n(2) First select the boss, then choose subset of $n$ that contains this boss: $n \\cdot {{n-1} \\choose {k-1}}$\nTheorem For $n\\geq 1$ we have: $$ 1{n \\choose 1} + 2{n \\choose 2} + \u0026hellip; + n{n \\choose n} = n2^{n-1} $$\nProof\n$n$ people, select a group of people with one boss. How many choices do we have?\n(1)\nSelect the size of the group: $i \\in \\set{1, \u0026hellip;, n}$ Select a group with $i$ people: $n \\choose i$ Inside the group, assign someone to be the boss: $i$ This gives us the left side of the identity.\n(2)\nChoose the boss: $n$ Chose the rest of the group: A subset of remainning people: $2^{n-1}$ Theorem For $n \\geq 0$ we have: $$ \\sum_{k=0}^{n}{n \\choose k}^2 = {{2n}\\choose n} $$ Right side: The number of choices for selecting a group of $n$ people out of $2n$ people.\nLabeling the people: $\\set{1, \u0026hellip;, 2n}$. Splitting this $2n$ people into two $n$ people group. Given a specific $k$, we can select $k$ people from the first group, and then $n-k$ people from the second group. In that situation, we have ${n \\choose k} \\cdot {n \\choose {n-k}}$. The final thing to do is simply take the sum of all the $k$\u0026rsquo;s. This gives us the left side of this identity.\nThe hockey-stick identity For $0 \\leq k \\leq n$ we have: $$ \\sum_{m=k}^{n}{m \\choose k} = {{n+1} \\choose {k+1}} $$ Intuition: Choose a column, and draw a box. Summing the values in the box gives the value at position $(n+1, k+1)$.\nProof\n$n+1$ people, select $k+1$: ${{n+1} \\choose {k+1}}$\nOrder people by height, from smallest to tallest: $1, 2, \u0026hellip;, n+1$\nSelect the tallest person in the group of size $m+1$, and then choose the rest $k$ people: ${{m} \\choose {k}}$ $m$ varies from $k$ to $n$ Summing all the $m$ cases gives us the left side of this identity. Reference: https://lmattos.web.illinois.edu/math-413-lecture-log/, MATH 413, Leticia Dias Mattos\n","permalink":"https://ohuro.me/posts/413_binomial/","summary":"Binomial Coefficient and Binomial Identity Pascal\u0026rsquo;s Triangle \\begin{matrix} {0 \\choose 0}\\\\ {1 \\choose 0} \u0026amp; {1 \\choose 1}\\\\ {2 \\choose 0} \u0026amp; {2 \\choose 1} \u0026amp; {2 \\choose 2}\\\\ {3 \\choose 0} \u0026amp; {3 \\choose 1} \u0026amp; {3 \\choose 2} \u0026amp; {3 \\choose 3}\\\\ {4 \\choose 0} \u0026amp; {4 \\choose 1} \u0026amp; {4 \\choose 2} \u0026amp; {4 \\choose 3} \u0026amp; {4 \\choose 4}\\\\ .\\\\ .\\\\ . \\end{matrix}\nPascal\u0026rsquo;s Formula For all integers $n$ and $k$ with $1 \\leq k \\leq n - 1$, we have: $$ {n \\choose k} = {{n-1} \\choose k} + {{n-1} \\choose {k-1}} $$","title":"413_binomial"},{"content":"Definition\nA combination of a set $S$ is a term usually used to denote an unordered selection of the element of $S$. It is simply a selection of a subset of $S$.\n$r$-combination = $r$-subset. $n\\choose r$ is the number of $r$-subset in the set of size $n$. Theorem For $0 \\leq r \\leq n$, we have: $$ P(n, r) = r! {n \\choose r} \\text{ and } {n \\choose r} = \\frac{n!}{r!(n-r)!} $$ Proof\nLet $S$ be a set of size $n$. $S = \\set{s_1, \u0026hellip;, s_n}$\nFirst, let us choose a string of size $r$ formed by elements of $S$ ($r$-permutation): $$ \\mathcal{P}(n, r) = \\set{(s_{i1}, \u0026hellip;, s_{ir}): s_{ij} \\in S, \\text{all distinct}} $$ And define: $$ \\mathcal{C}(n, r) = \\text{set of subsets of }S \\text{ with size }r $$ Define the function: $$ f:\\mathcal{P}(n, r) \\to \\mathcal{C}(n, r)\\ f(s_{i1}, \u0026hellip;, s_{ir}) = \\set{s_{i1}, \u0026hellip;, s_{ir}} $$ Consider: How many times a set is being counted?\nAns: $r!$\nThen we have: $$ |\\mathcal{P}(n, r)| = r!|\\mathcal{C}(n, r)| $$\nCorollary\n$$ {n \\choose r} = {n \\choose {n-r}} $$\nExercise\nThere is a $4 \\times 6$ grid. Starting from bottom left and ending at top right. You can only choose up or right move. How many different paths can you take?\nThere is a bijection between these paths and the set of strings of size 10 of the form: $(a_1, a_2,\u0026hellip;, a_{10})$, where each $a_i$ is $U$ or $R$. ($URUURR\u0026hellip;$). But since the number of $U$\u0026rsquo;s = 4, the number of distributing 4 $U$\u0026rsquo;s in a string of size 10 is $10 \\choose 4$.\nTheorem Let $S$ be a multiset with objects of $k$ different types, where each object has an infinite repetition number. Then, the number of $r$-permutations of $S$ is $k^r$.\nProof\nThere are $r$ position to be filled in. Each position has $k$ choices since we have infinite number of each type in this multiset S. This gives us $k^r$\nAnother examle: $S = \\set{a, a, b}$. If we want 3-permutation of S. In this case, $(a, a, a)$ is not a 3-permutation of S.\nTheorem Let the size of $S$ be $n = n_1 + \u0026hellip; + n_k$. Then the number of permutations of $S$ is equal to: $$ \\frac{n!}{(n_1)!(n_2)!\u0026hellip; (n_k)! } $$\nProof\nLet $a_1^1, \u0026hellip;, a_{n_1}^1$ be a labelling of the elements of type 1.\nLet $a_1^2, \u0026hellip;, a_{n_2}^2$ be a labelling of the elements of type 2.\nLet $a_1^3, \u0026hellip;, a_{n_3}^3$ be a labelling of the elements of type 3.\n\u0026hellip;\nLet $a_1^k, \u0026hellip;, a_{n_k}^k$ be a labelling of the elements of type 3.\nConsider the set $S\u0026rsquo; = \\set{a_1^1, \u0026hellip;, a_{n_1}^1, \u0026hellip;, a_1^k, \u0026hellip;, a_{n_k}^k}$\nThe permutations of $S\u0026rsquo; = n!$. Then we can create a function: $$ f:\\set{\\text{permutations of }S\u0026rsquo;} \\to {\\text{permutation of }S}\\ f(\\text{string} = \\text{string where the labels are removed}) $$ Removing the label makes all those elements in the same type indistinguishable. (You cannot tell the different between $a_1^1$ and $a_2^1$ if the lable is removed.)\nConsider: For a given permutation $A$ of $S$, what\u0026rsquo;s the number of the set $\\set{S: f(S) = A}$?\nAns: $(n_1)!(n_2)! \u0026hellip; (n_k)!$\nExercise\nHow many ways do we have of partitioning the set $\\set{1, 2, \u0026hellip;, 10}$ into 3 sets: out of size 2, out of size 3, and another of size 5?\nSolution 1: $$ {10 \\choose 2} {8 \\choose 3} {5 \\choose 5} = {10 \\choose 5} {5 \\choose 3} {2 \\choose 2} $$ Solution 2:\nDefine the set: $\\mathcal{P}(10) =$ set of all permutations of $\\set{1, \u0026hellip;, 10}$. Then, $|\\mathcal{P} = 10!|$\nDefine: $f: \\mathcal{P} \\to \\set{\\text{Box configurations}}$ by: $$ f(a_1, \u0026hellip;, a_{10}) = \\left( \\set{a_1, a_2}, {a_3, a_4, a_5}, {\u0026hellip;} \\right) $$\nQuestion: Given a box configuration $\\mathcal{B}$, what is the number $\\set{S:f(S) = \\mathcal{B}}$, where $S$ are strings?\n$$ |\\mathcal{P}(10)| = |\\text{Box configuration}| \\cdot 2! \\cdot 3! \\cdot 5! $$ This gives us: $$ |\\text{Box configuration}| = \\frac{|\\mathcal{P}(10)|}{2! \\cdot 3! \\cdot 5!} $$\nTheorem 2.4.3 Let $n$ be a positive integer and let $n_1, n_2, \u0026hellip; ,n_k$ be positive integers with $n = n_l + n_2 + \u0026hellip; + n_k$· The number of ways to partition a set of $n$ objects into k labeled boxes in which Box 1 contains $n_1$ objects, Box 2 contains $n_2$ objects, \u0026hellip; , Box $k$ contains $n_k$ objects equals: $$ \\frac{n!}{n_1! \\cdot n_2! \\cdot \\cdot \\cdot n_k!} $$\nTheorem 2.5.1 Let $S$ be a multiset with objects of $k$ types, each with an infinite repetition number. Then, the number of $r$-combinations of $S$ equals to: $$ {{r + k - 1} \\choose r} $$ Proof\nNow: we need to select a submultiset. Each submultiset is associated to a solution of:\n\\begin{align*} X_1 + ... + X_k \u0026amp;= r\\\\ X_1, ..., X_k \u0026amp;\\in \\mathbb{N} \\cup \\set{0} \\end{align*} Let $\\mathcal{S}$ be the set of solutions.\nDefine: $f: \\mathcal{S} \\to \\set{0, 1 \\text{ bit strings with } k - 1 \\text{ ones and }r \\text{ zeros}}$ by :\n$f(x_1, \u0026hellip;, x_k) = (00 \u0026hellip;0(X_1\\text{ zeros})) 1 (00 \u0026hellip;0(X_2\\text{ zeros})) 1 \u0026hellip; (00 \u0026hellip;0(X_k\\text{ zeros}))$\nGiven $s \\in f(\\mathcal{S})$, the number of 1\u0026rsquo;s in $s = k - 1$, and the number of 0\u0026rsquo;s in $s = r$.\nEasy to check that this function $f$ is a bijection.\n$\\mathcal{R} =$ set of 0-1 strings with $k-1$ 1\u0026rsquo;s and $r$ 0\u0026rsquo;s. It suffices to show: $|\\mathcal{R}| = |\\mathcal{S}|$. $\\blacksquare$\nExercise\nA bakery boasts eight varieties of doughnuts. If a box of doughnuts contains one dozen, how many different options are there for a box of doughnuts?\nWe need to calculate the number of solutions for: $X_1 + \u0026hellip; + X_8 = 12$, which is: ${8 + 12 - 1} \\choose 12$.\nExercise\nWhat is the number of integral solutions of the equation $X_1 + X_2 + X_3 + X_4 = 20$, in which $X_1 \\geq 3, X_2 \\geq 1, X_3 \\geq 0, X_4 \\geq 5$?\nDefine:\n\\begin{align*} X_1 \u0026amp;= Y_1 + 3\\\\ X_2 \u0026amp;= Y_2 + 1\\\\ X_3 \u0026amp;= Y_3\\\\ X_4 \u0026amp;= Y_4 + 5\\\\ \\end{align*}\nThis gives us: $Y_1 + Y_2 + Y_3 + Y_4 = 20 - (3 + 1 + 0 + 5) = 11$\nThe number of solutions with $Y_i \\in \\mathbb{N} \\cup \\set{0} = {14 \\choose 3}$\nReference: https://lmattos.web.illinois.edu/math-413-lecture-log/, MATH 413, Leticia Dias Mattos\n","permalink":"https://ohuro.me/posts/413_combinations/","summary":"Definition\nA combination of a set $S$ is a term usually used to denote an unordered selection of the element of $S$. It is simply a selection of a subset of $S$.\n$r$-combination = $r$-subset. $n\\choose r$ is the number of $r$-subset in the set of size $n$. Theorem For $0 \\leq r \\leq n$, we have: $$ P(n, r) = r! {n \\choose r} \\text{ and } {n \\choose r} = \\frac{n!","title":"413_combinations"},{"content":"Deterministic Finite Automaition Definition: $(Q, s, A, \\delta)$\n$Q$: A set of all the possible state.\n$s \\in Q$ : starting state. The state to start the machine.\n$A \\subseteq Q$: The set of states we accept. Return \u0026ldquo;good\u0026rdquo; when we end there.\n$\\delta$: A function $Q \\times \\Sigma \\rightarrow Q$. The set of all transition functions.\nThere is an interesting DFA design question. The question is as follows:\nGiven a bit-string (a string that contains only 0s and 1s), design a DFA that accepts the string if and only if the number represented by the string is divisible by 5.\nInitially, I was thinking of the trailing bits of the binary representation. However, there are too many states to consider. A better way of constructing the states of this machine is to consider the reminders. By doing this, it only uses 5 states \u0026mdash; it worth a try. So, I tried to write out the binary numbers and tried to design the transition functions for simple cases. It turns out that the result works well.\nI also wrote a short python script for verifying this:\n# verifying dfa # for this specific task, we need to use DFA to determine wheter the incomming bit string can be divible by 5. # The general definition of a DFA is: # M = (Q, q0, A, S) # Q is a finite set of states # q0 is the start state # A is a set of accepting states # delta is a transition function that takes a state and an input symbol and returns a state # also consider the alphabet sigma, we can use this to generate random inputs sigma = [0, 1] # this is the alphabet. # We need to define the machine, and then we can verify it Q = [0, 1, 2, 3, 4] q0 = 0 A = [0] # represent the transition function as a dictionary # the key is a tuple of (state, input symbol) # the value is the next state # the size of this dictionay == |Q| * |sigma| delta = { (0, 0): 0, (0, 1): 1, (1, 0): 2, (1, 1): 3, (2, 0): 4, (2, 1): 0, (3, 0): 1, (3, 1): 2, (4, 0): 3, (4, 1): 4 } # now we can write a function to verify the DFA # we assume that the given dfa is an tuple of (Q, q0, A, S) def verify_dfa(dfa, input_string): # first we need to check if the input_string is valid for c in input_string: if (int)(c) not in sigma: return False # start the verification current_state = dfa[1] for c in input_string: current_state = dfa[3][(current_state, (int)(c))] # looking up the dict for the next state if current_state in dfa[2]: return True return False # end of verify_dfa # now we can test the function dfa = (Q, q0, A, delta) import random # randomly generate strings of length less than length l test_num = 100 l = 10 result = True for _ in range(test_num): length = random.randint(1, l) input_string = \u0026#39;\u0026#39; for _ in range(length): input_string += str(random.randint(0, 1)) if (int(input_string, 2) % 5 == 0) != verify_dfa(dfa, input_string): result = False break # print in the order: dfa result, actual resultinput string in decimal, seperated by tab # print(verify_dfa(dfa, input_string), end=\u0026#39;\\t\u0026#39;) # print(int(input_string, 2) % 5 == 0, end=\u0026#39;\\t\u0026#39;) # print(int(input_string, 2), end=\u0026#39;\\n\u0026#39;) # end of for print(f\u0026#34;tested {test_num} times with bit string of length less than {l}, \u0026#34;, end=\u0026#34;\u0026#34;) if result: print(\u0026#34;the DFA is correct!\u0026#34;) else: print(\u0026#34;the DFA is not correct!\u0026#34;) ","permalink":"https://ohuro.me/posts/interesting_dfa/","summary":"Deterministic Finite Automaition Definition: $(Q, s, A, \\delta)$\n$Q$: A set of all the possible state.\n$s \\in Q$ : starting state. The state to start the machine.\n$A \\subseteq Q$: The set of states we accept. Return \u0026ldquo;good\u0026rdquo; when we end there.\n$\\delta$: A function $Q \\times \\Sigma \\rightarrow Q$. The set of all transition functions.\nThere is an interesting DFA design question. The question is as follows:\nGiven a bit-string (a string that contains only 0s and 1s), design a DFA that accepts the string if and only if the number represented by the string is divisible by 5.","title":"Interesting DFA"},{"content":"Gomory\u0026rsquo;s Theorem If you move two cells of a $8 \\times 8$ chessboard of opposite colors, the remaining cells can be fully domino tiled.\nProof\nDraw a closed path that passes through every square exactly once. (Draw a big C and then draw back and forth horizontally)\nChoose the two cells to be removed, and the closed path we have will be sperated into two paths.\nIf we lable the close path we chose in the beginning from 1 to 64 in the order we drew it, white cells have odd numbers, and black cells have even numbers (or reversed).\nEasy to see that the path in between the two cells crosses even number tiles.\nGomory\u0026rsquo;s on more general chessboard ($n \\times m$ board) The two opposite corners must be in opposite color iff n and m has different paritity.\nProof\nConsider the \u0026ldquo;L\u0026rdquo; shape edge of the board(half the square), there are $m + n - 1$ tiles. When this number is even, they are in opposite color.\nThen check: When m and n have different parity, then there must be an \u0026ldquo;S\u0026rdquo; shape closed path to cross all the tiles.\nDefinition\nb-ominos. It covers b tiles in a row\u0026hellip;\nTheorem An $m \\times n$ board has a tiling with b-ominos iff $b|m$ or $b|n$.\nProof\n$\\leftarrow$ is trivial.\n$\\rightarrow$:\nColor the board: \\begin{matrix} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp; 5 \u0026amp; 6 \u0026amp;...\\\\ 3 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp; 5 \u0026amp;... \\\\ 2 \u0026amp; 3 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \u0026amp;... \\\\ .\\\\ .\\\\ . \\end{matrix} Consider the board has dimension $m \\times n$ and $m = qb + r$, $n = tb + s$\nThen, on this board, each b-omino see a color exactly once.\nAssume, for contradiction, $r, s \\neq 0$\nAs we are assuming to have a b-omino tiling, each color shows up the same number of times $\\rightarrow$ then in the little $r \\times s$ chessboard, each color has the same number of squares. WLOG, assume $s \\leq r$. The number of 1\u0026rsquo;s in the $r \\times s$ chessboard = $s$. The total number of squares is $rs$. Consider we tiling with b-ominos, the number of sqares is also $sb$ (Each b-omino sees every color once, so number of 1\u0026rsquo;s = number of tiles. So $sb$). Cancle out the b, we have the contradiction.\nReference: https://lmattos.web.illinois.edu/math-413-lecture-log/, MATH 413, Leticia Dias Mattos\n","permalink":"https://ohuro.me/posts/413_chessboard/","summary":"Gomory\u0026rsquo;s Theorem If you move two cells of a $8 \\times 8$ chessboard of opposite colors, the remaining cells can be fully domino tiled.\nProof\nDraw a closed path that passes through every square exactly once. (Draw a big C and then draw back and forth horizontally)\nChoose the two cells to be removed, and the closed path we have will be sperated into two paths.\nIf we lable the close path we chose in the beginning from 1 to 64 in the order we drew it, white cells have odd numbers, and black cells have even numbers (or reversed).","title":"413_chessboard"},{"content":"Consider a $n \\times m$ chessboard\u0026hellip;\n$$ \\int{f(x)dx} $$\nAnthony\u0026rsquo;s test: $\\mathcal{F}_k \\subseteq \\mathcal{F}_{k+1}$\nSince $94 = 4 + 5x$ for some $x \\in \\mathbb{N}$, my claim is that the first player wins when it chooses $4$ in the beginning. Then, whenever the second player choose $a, x \\in\\set{1, 2, 3, 4}$, the first player just add it to 5. So choose $5-a$. By doing this, the first player always reaches $10x + 9$ or $10x + 4$ for $x \\in \\mathbb{N}$. Since $94$ is included, the first player must win.\n$\\mathfrak{ABCDEFG}$\n$\\mathscr{ABCDEFG}$\nHi $z = x + y$.\n$$a^2 + b^2 = c^2$$\n$$\\begin{vmatrix}a \u0026amp; b\\\\ c \u0026amp; d \\end{vmatrix}=ad-bc$$\nfrom: https://yihui.org/en/2018/07/latex-math-markdown/\ncomplex math (array) $$ \\begin{array} {lcl} L(p,w_i) \u0026amp;=\u0026amp; \\dfrac{1}{N}\\Sigma_{i=1}^N(\\underbrace{f_r(x_2 \\rightarrow x_1 \\rightarrow x_0)G(x_1 \\longleftrightarrow x_2)f_r(x_3 \\rightarrow x_2 \\rightarrow x_1)}_{sample\\, radiance\\, evaluation\\, in\\, stage2} \\\\\\\\\\\\ \u0026amp;=\u0026amp; \\prod_{i=3}^{k-1}(\\underbrace{\\dfrac{f_r(x_{i+1} \\rightarrow x_i \\rightarrow x_{i-1})G(x_i \\longleftrightarrow x_{i-1})}{p_a(x_{i-1})}}_{stored\\,in\\,vertex\\, during\\,light\\, path\\, tracing\\, in\\, stage1})\\dfrac{G(x_k \\longleftrightarrow x_{k-1})L_e(x_k \\rightarrow x_{k-1})}{p_a(x_{k-1})p_a(x_k)}) \\end{array} $$\nTesting alignments $$ \\begin{align*} \\frac{1}{\\Gamma(s)}\\int_{0}^{\\infty}\\frac{u^{s-1}}{e^{u}-1}\\mathrm{d}u \\end{align*} $$\nHx tests $$ \\mathcal{F} $$\n$$ \\lambda_i \\xlongequal[]{\\text{eq.}(1)}\\int_{\\set{i}} $$\n$$ \\begin{align*} \\text{(1) } 0\\oplus 0 \u0026amp; = 0\\\\ \\text{(2) } 1\\oplus 0 \u0026amp; = 1\\\\ \\text{(3) } 0\\oplus 1 \u0026amp; = 1\\\\ \\text{(4) } 1\\oplus 1 \u0026amp; = 0 \\end{align*} $$\nWe observe that:\n$$ \\begin{itemize} \\item for $y=x\\oplus z$, $y$ agreees with $x$ if (1) or (3) happen; disagrees if (2) or (4) happen. \\item $y=0$ iff $x$ and $z$ agree; $y=1$ iff $x$ and $z$ disagree. \\end{itemize} $$\nTheorem Proof\nDefinition\nNotation\nExercise\nProposition\nCorollary\nObservation\nIf you need these source codes:\nConsider a $n \\times m$ chessboard... $$ \\int{f(x)dx} $$ Since $94 = 4 + 5x$ for some $x \\in \\mathbb{N}$, my claim is that the first player wins when it chooses $4$ in the beginning. Then, whenever the second player choose $a, x \\in\\set{1, 2, 3, 4}$, the first player just add it to 5. So choose $5-a$. By doing this, the first player always reaches $10x + 9$ or $10x + 4$ for $x \\in \\mathbb{N}$. Since $94$ is included, the first player must win. $\\mathfrak{ABCDEFG}$ Hi `$z = x + y$`. `$$a^2 + b^2 = c^2$$` `$$\\begin{vmatrix}a \u0026amp; b\\\\ c \u0026amp; d \\end{vmatrix}=ad-bc$$` from: https://yihui.org/en/2018/07/latex-math-markdown/ ### complex math (array) `$$ \\begin{array} {lcl} L(p,w_i) \u0026amp;=\u0026amp; \\dfrac{1}{N}\\Sigma_{i=1}^N(\\underbrace{f_r(x_2 \\rightarrow x_1 \\rightarrow x_0)G(x_1 \\longleftrightarrow x_2)f_r(x_3 \\rightarrow x_2 \\rightarrow x_1)}_{sample\\, radiance\\, evaluation\\, in\\, stage2} \\\\\\\\\\\\ \u0026amp;=\u0026amp; \\prod_{i=3}^{k-1}(\\underbrace{\\dfrac{f_r(x_{i+1} \\rightarrow x_i \\rightarrow x_{i-1})G(x_i \\longleftrightarrow x_{i-1})}{p_a(x_{i-1})}}_{stored\\,in\\,vertex\\, during\\,light\\, path\\, tracing\\, in\\, stage1})\\dfrac{G(x_k \\longleftrightarrow x_{k-1})L_e(x_k \\rightarrow x_{k-1})}{p_a(x_{k-1})p_a(x_k)}) \\end{array} $$` ### Testing alignments `$$ \\begin{align*} \\frac{1}{\\Gamma(s)}\\int_{0}^{\\infty}\\frac{u^{s-1}}{e^{u}-1}\\mathrm{d}u \\end{align*} $$` ### Hx tests 1. $$ \\mathcal{F} $$ 2. $$ \\lambda_i \\xlongequal[]{\\text{eq.}(1)}\\int_{\\set{i}} $$ 3. `$$ \\begin{align*} \\text{(1) } 0\\oplus 0 \u0026amp; = 0\\\\ \\text{(2) } 1\\oplus 0 \u0026amp; = 1\\\\ \\text{(3) } 0\\oplus 1 \u0026amp; = 1\\\\ \\text{(4) } 1\\oplus 1 \u0026amp; = 0 \\end{align*} $$` We observe that: 4. $$ \\begin{itemize} \\item for $y=x\\oplus z$, $y$ agreees with $x$ if (1) or (3) happen; disagrees if (2) or (4) happen. \\item $y=0$ iff $x$ and $z$ agree; $y=1$ iff $x$ and $z$ disagree. \\end{itemize} $$ ","permalink":"https://ohuro.me/posts/latex_test/","summary":"Consider a $n \\times m$ chessboard\u0026hellip;\n$$ \\int{f(x)dx} $$\nAnthony\u0026rsquo;s test: $\\mathcal{F}_k \\subseteq \\mathcal{F}_{k+1}$\nSince $94 = 4 + 5x$ for some $x \\in \\mathbb{N}$, my claim is that the first player wins when it chooses $4$ in the beginning. Then, whenever the second player choose $a, x \\in\\set{1, 2, 3, 4}$, the first player just add it to 5. So choose $5-a$. By doing this, the first player always reaches $10x + 9$ or $10x + 4$ for $x \\in \\mathbb{N}$.","title":"Some latex tests"},{"content":"I have had a wonderful summer as I have been able to take Math 417 with Professor Chales Rezk. He is a very very good teacher and I have learnt a lot from him. Thanks!\nThese are the notes I have taken during the course. It includes the greate Theorems, Lemmas and Propositions that we have learnt in the course.\nPrevious Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here. ","permalink":"https://ohuro.me/posts/417_review/","summary":"I have had a wonderful summer as I have been able to take Math 417 with Professor Chales Rezk. He is a very very good teacher and I have learnt a lot from him. Thanks!\nThese are the notes I have taken during the course. It includes the greate Theorems, Lemmas and Propositions that we have learnt in the course.\nPrevious Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here.","title":"My Math 417 review notes"},{"content":"The four basic counting principle Suppose that a set $S$ is partitioned into pairwise disjoint parts $S_1, S_2, \u0026hellip;, S_n$.\nAddition principle: $$ |S| = |S_1| + |S_2| + \u0026hellip; + |S_m| $$\nEx:\nPath counting: In a $3 \\times 3$ grid, if you can move 1 step upward or 1 step to the right. How many ways do we have to move from bottom-left to top-right corner?\nThe idea is to break this problem into smaller problems. Addition principle guarrantees that we can add the numbers together.\nThink of the final step we need to take. It must be either an up or a right. This makes the problem to become a similar but smaller problem that has $2 \\times 3$ size. Mark it $n_{2\\times 3}$\nUsing the same idea, we have: \\begin{align*} n_{3 \\times 3} = 2 * n_{2 \\times 3} = 2 * (n_{2 \\times 2} + n_{3 \\times 1})\\\\ \\end{align*}\nMultiplication principle Let $S$ be a set of ordered pairs $(a, b)$ of objects, where the first object $a$ comes from a set of size $p$, and for each choice of object $a$ there are q choices for object $b$. Then the size of $S$ is $$ |S| = p \\cdot q $$\nGeneralized multiplicative principle\nLet $S$ be a finite set of $p$-tuples $a = (a_1, \u0026hellip;, a_p)$ and let $t_1, \u0026hellip;, t_p$ be some positive numbers.\nSuppose that for every $a = (a_1, \u0026hellip;, a_p) \\in S$, and every $i \\in \\set{1, \u0026hellip;, p}$, we have $$ |\\set{b \\in S:b_j = a_j, \\forall j \\neq i}| = t_i $$ Then, $$ |S| = t_1 \\cdot t_2 \\cdot \u0026hellip; t_p $$ (4) counts the number of choices if we fix all other dimension except $i$.\nEx:\nDetermine the number of positive integers that are factors of the number $2^{10}\\cdot 7 \\cdot 13^3$.\nThis number can be written as: $2^{a}\\cdot 7^b \\cdot 13^c$. Then just consider the number of choices for $a, b, c$:\n$a \\in \\set{0, 1, \u0026hellip; 10}$ $b \\in \\set{0, 1}$ $c \\in \\set{0, 1, 2, 3}$\nIn total, $11 \\cdot 2 \\cdot 4 = 88$\nObservation In the multiplication principle the $q$ choices for object $b$ may vary the choice of $a$. The only requirement is that there be the same number $q$ of choices, not necessarily the same choices.\n(When we choose one coordinate, the choices for other coordinates changes)\nEx:\nHow many two-digit numbers have distinct and nonzero digits?\n9 * 8 = 72. The first digit has 9 choices, the second digit has 8 (since 0 and the 1st choice are not allowed)\nSubtraction Principle Let $A$ be a set and let $U$ be a larger set containing $A$. Let $$ \\bar{A} = U \\setminus A = \\set{x \\in U : x \\notin A} $$\nEx:\nComputer passwords are consist of a string of six symbols taken from digit 0 to 9 and letters a to z.\nHow many computer passwords have a repeat symbol?\nans: The number of total passwords - passwords that don\u0026rsquo;t repeat.\n= $36^6 - 36 * 35 * 34 * 33 * 32 * 31$\nDivision principle Let $S$ be a finite set htat is partitioned into k parts in such a way that each part contains the same number of objects. Then the number of parts in the partition is given by the rule. $$ k = \\frac{|S|}{|S_i|} $$ A multiset is a modification of the concept of a set that, unlike a set, allows for multiple instances for each of its elements.\nEx: $\\set{a, a, b}$ the element a has multiplicity 2, and b has multiplicity 1.\nDivision principle for multisets\nLet $S$ be a finite multiset in which each element has the same multiplicity $m$, Let $A$ be the underlying set of $S$, formed from its distinct elements. The the size is given by: $$ |A| = \\frac{|S|}{m} $$\nReference: https://lmattos.web.illinois.edu/math-413-lecture-log/, MATH 413, Leticia Dias Mattos\n","permalink":"https://ohuro.me/posts/413_basic_counting/","summary":"The four basic counting principle Suppose that a set $S$ is partitioned into pairwise disjoint parts $S_1, S_2, \u0026hellip;, S_n$.\nAddition principle: $$ |S| = |S_1| + |S_2| + \u0026hellip; + |S_m| $$\nEx:\nPath counting: In a $3 \\times 3$ grid, if you can move 1 step upward or 1 step to the right. How many ways do we have to move from bottom-left to top-right corner?\nThe idea is to break this problem into smaller problems.","title":"413_basic counting"},{"content":"","permalink":"https://ohuro.me/about/","summary":"","title":""}]